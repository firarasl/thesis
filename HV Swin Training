import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import json
from datetime import datetime
import random
warnings.filterwarnings('ignore')

# Install timm for Swin Transformer
try:
    import timm
    print("timm is already installed")
except ImportError:
    print("Installing timm...")
    import subprocess
    subprocess.run(['pip', 'install', 'timm'])
    import timm

# Set seeds for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class DiabeticRetinopathyHVChannelDataset(Dataset):
    """
    Custom Dataset for Diabetic Retinopathy with HV (Hue-Value) channels only
    """
    def __init__(self, csv_file, img_dir, transform=None, is_test=False):
        """
        Args:
            csv_file (string): Path to csv with image names and labels
            img_dir (string): Directory with all images
            transform (callable, optional): Transform to be applied on images
            is_test (bool): Whether this is test dataset (no labels)
        """
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        self.is_test = is_test
        
        # Print dataset statistics
        if not is_test:
            print(f"HV Channel Dataset loaded: {len(self.data)} images")
            print(f"Output channels: 2 (Hue + Value)")
            label_counts = self.data.iloc[:, 1].value_counts().sort_index()
            print("Label distribution:")
            for label, count in label_counts.items():
                print(f"  Class {label}: {count} images ({count/len(self.data)*100:.1f}%)")
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name
        img_name = self.data.iloc[idx, 0]
        
        # Try different extensions
        img_extensions = ['.jpeg', '.jpg', '.png']
        img_path = None
        
        for ext in img_extensions:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension")
        
        # Load image
        image = cv2.imread(img_path)
        if image is None:
            raise ValueError(f"Could not load image: {img_path}")
        
        # Convert BGR to RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Convert RGB to HSV
        image_hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)
        
        # Extract Hue and Value channels (H and V)
        image_hv = image_hsv[:, :, [0, 2]]  # H, V channels
        
        # Convert to PIL Image for transforms
        image_pil = Image.fromarray(image_hv.astype('uint8'))
        
        if self.transform:
            image_pil = self.transform(image_pil)
            
        if not self.is_test:
            label = int(self.data.iloc[idx, 1])
            return image_pil, label
        else:
            return image_pil, img_name

class HVSwinTransformerClassifier(nn.Module):
    """
    Diabetic Retinopathy Classifier for HV (Hue-Value) channels using Swin Transformer
    """
    def __init__(self, num_classes=5, model_name='swin_base_patch4_window7_224', pretrained=True, dropout_rate=0.5):
        super(HVSwinTransformerClassifier, self).__init__()
        
        # Use Swin Transformer as backbone
        self.backbone = timm.create_model(
            model_name, 
            pretrained=pretrained, 
            num_classes=0,  # Remove classification head
            in_chans=2      # HV has 2 channels
        )
        
        # Get feature dimension
        num_features = self.backbone.num_features
        
        # Custom classifier head
        self.classifier = nn.Sequential(
            nn.LayerNorm(num_features),
            nn.Dropout(dropout_rate),
            nn.Linear(num_features, 512),
            nn.GELU(),
            nn.Dropout(dropout_rate/2),
            nn.Linear(512, 256),
            nn.GELU(),
            nn.Dropout(dropout_rate/4),
            nn.Linear(256, num_classes)
        )
        
        # Initialize weights for new layers
        self._init_weights(self.classifier)
        
        print(f"HV Swin Transformer Model initialized with {sum(p.numel() for p in self.parameters())} parameters")
        print(f"Using Swin Transformer variant: {model_name}")
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.LayerNorm):
            nn.init.constant_(module.bias, 0)
            nn.init.constant_(module.weight, 1.0)
        
    def forward(self, x):
        features = self.backbone(x)
        return self.classifier(features)

def calculate_hv_normalization_stats(csv_file, img_dir, sample_size=500):
    """
    Calculate proper normalization statistics for HV channels from actual data
    """
    print("Calculating HV channel normalization statistics...")
    data = pd.read_csv(csv_file)
    
    # Sample images for statistics calculation
    sample_indices = np.random.choice(len(data), min(sample_size, len(data)), replace=False)
    
    all_pixels_h = []
    all_pixels_v = []
    
    for idx in tqdm(sample_indices, desc="Computing HV stats"):
        img_name = data.iloc[idx, 0]
        
        # Try different extensions
        img_path = None
        for ext in ['.jpeg', '.jpg', '.png']:
            potential_path = os.path.join(img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
        
        if img_path is None:
            continue
            
        try:
            # Load image
            image = cv2.imread(img_path)
            if image is None:
                continue
                
            # Convert BGR to RGB
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Convert RGB to HSV
            image_hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)
            
            # Resize to standard size for consistent statistics
            image_resized = cv2.resize(image_hsv, (224, 224))
            
            # Extract Hue and Value channels and normalize to [0,1]
            hue_channel = image_resized[:, :, 0].flatten() / 179.0  # Hue range: 0-179
            value_channel = image_resized[:, :, 2].flatten() / 255.0  # Value range: 0-255
            
            # Sample pixels to avoid memory issues
            sample_pixels = min(1000, len(hue_channel))
            indices = np.random.choice(len(hue_channel), sample_pixels, replace=False)
            
            all_pixels_h.extend(hue_channel[indices])
            all_pixels_v.extend(value_channel[indices])
            
        except Exception as e:
            print(f"Error processing {img_name}: {e}")
            continue
    
    if not all_pixels_h or not all_pixels_v:
        print("Warning: No valid images found, using default normalization")
        return [0.5, 0.5], [0.3, 0.3]
    
    # Calculate statistics
    mean_h = np.mean(all_pixels_h)
    mean_v = np.mean(all_pixels_v)
    std_h = np.std(all_pixels_h)
    std_v = np.std(all_pixels_v)
    
    # Ensure std is not too small
    std_h = max(std_h, 0.1)
    std_v = max(std_v, 0.1)
    
    print(f"HV Channel Statistics (from {len(sample_indices)} images):")
    print(f"  Hue Channel - Mean: {mean_h:.4f}, Std: {std_h:.4f}")
    print(f"  Value Channel - Mean: {mean_v:.4f}, Std: {std_v:.4f}")
    
    return [mean_h, mean_v], [std_h, std_v]

def get_hv_swin_transforms(hv_mean, hv_std, is_training=False):
    """
    Define data augmentation and preprocessing transforms for HV channels with Swin Transformer
    """
    if is_training:
        transform = transforms.Compose([
            transforms.Resize((224, 224)),  # Swin Transformer typically uses 224x224
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),
            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
            transforms.ToTensor(),
            transforms.Normalize(mean=hv_mean, std=hv_std)
        ])
    else:
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=hv_mean, std=hv_std)
        ])
    
    return transform

def quadratic_weighted_kappa(y_true, y_pred):
    """Calculate Quadratic Weighted Kappa (QWK) score"""
    return cohen_kappa_score(y_true, y_pred, weights='quadratic')

def train_hv_swin_model(model, train_loader, val_loader, num_epochs=60, learning_rate=1e-4, model_name="hv_swin_model"):
    """
    Train HV channel model with Swin Transformer backbone
    """
    # Loss function
    criterion = nn.CrossEntropyLoss()
    
    # Optimizer with different learning rates for backbone and classifier
    backbone_params = []
    classifier_params = []
    
    for name, param in model.named_parameters():
        if 'backbone' in name:
            backbone_params.append(param)
        else:
            classifier_params.append(param)
    
    optimizer = optim.AdamW([
        {'params': backbone_params, 'lr': learning_rate * 0.1},  # Lower LR for backbone
        {'params': classifier_params, 'lr': learning_rate}       # Higher LR for classifier
    ], weight_decay=1e-4, betas=(0.9, 0.999))
    
    # Learning rate scheduler
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )
    
    # Training history
    history = {
        'train_losses': [],
        'val_losses': [],
        'val_kappas': [],
        'val_accuracies': [],
        'learning_rates': []
    }
    
    best_kappa = 0.0
    best_model_state = None
    patience_counter = 0
    patience = 15
    
    print(f"\n{'='*70}")
    print("STARTING HV CHANNEL SWIN TRANSFORMER MODEL TRAINING")
    print(f"{'='*70}")
    print(f"Training samples: {len(train_loader.dataset)}")
    print(f"Validation samples: {len(val_loader.dataset)}")
    print(f"Batch size: {train_loader.batch_size}")
    print(f"Learning rate: {learning_rate}")
    print(f"{'='*70}")
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [HV Swin] Train')
        
        for batch_idx, (data, target) in enumerate(train_pbar):
            data, target = data.to(device), target.to(device)
            
            # Check for NaN or invalid data
            if torch.isnan(data).any() or torch.isinf(data).any():
                print(f"Warning: NaN/Inf detected in input data at batch {batch_idx}")
                continue
            
            optimizer.zero_grad()
            
            try:
                output = model(data)
                loss = criterion(output, target)
                
                # Check for NaN loss
                if torch.isnan(loss):
                    print(f"Warning: NaN loss at batch {batch_idx}")
                    continue
                    
                loss.backward()
                
                # Gradient clipping for transformers
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                train_correct += pred.eq(target.view_as(pred)).sum().item()
                train_total += target.size(0)
                
                current_acc = 100. * train_correct / train_total
                train_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })
                
            except Exception as e:
                print(f"Error in training batch {batch_idx}: {e}")
                continue
        
        if train_total == 0:
            print("No valid training batches processed!")
            break
            
        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = 100. * train_correct / train_total
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [HV Swin] Val')
            for batch_idx, (data, target) in enumerate(val_pbar):
                data, target = data.to(device), target.to(device)
                
                try:
                    output = model(data)
                    loss = criterion(output, target)
                    val_loss += loss.item()
                    
                    pred = output.argmax(dim=1, keepdim=True)
                    val_correct += pred.eq(target.view_as(pred)).sum().item()
                    val_total += target.size(0)
                    
                    all_preds.extend(pred.cpu().numpy().flatten())
                    all_targets.extend(target.cpu().numpy())
                    
                    current_acc = 100. * val_correct / val_total
                    val_pbar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Acc': f'{current_acc:.2f}%'
                    })
                    
                except Exception as e:
                    print(f"Error in validation batch {batch_idx}: {e}")
                    continue
        
        if val_total == 0:
            print("No valid validation batches processed!")
            break
            
        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100. * val_correct / val_total
        val_kappa = quadratic_weighted_kappa(all_targets, all_preds)
        
        # Update learning rate
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # Store history
        history['train_losses'].append(avg_train_loss)
        history['val_losses'].append(avg_val_loss)
        history['val_kappas'].append(val_kappa)
        history['val_accuracies'].append(val_accuracy)
        history['learning_rates'].append(current_lr)
        
        # Print epoch results
        print(f'\nEpoch {epoch+1}/{num_epochs} [HV Swin]:')
        print(f'  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%')
        print(f'  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%')
        print(f'  Val QWK: {val_kappa:.4f}')
        print(f'  Learning Rate: {current_lr:.8f}')
        
        # Save best model
        if val_kappa > best_kappa:
            best_kappa = val_kappa
            best_model_state = model.state_dict().copy()
            torch.save({
                'model_state_dict': best_model_state,
                'epoch': epoch,
                'best_kappa': best_kappa,
                'history': history
            }, f'best_{model_name}.pth')
            patience_counter = 0
            print(f'  ‚úÖ New best HV Swin model saved! QWK: {best_kappa:.4f}')
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f'\nEarly stopping triggered after {patience} epochs without improvement.')
            break
            
        print('-' * 70)
    
    # Load best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nLoaded best HV Swin model with QWK: {best_kappa:.4f}")
    
    history['best_kappa'] = best_kappa
    history['total_epochs'] = epoch + 1
    
    return model, history

def create_hv_data_loaders(train_csv, img_dir, hv_mean, hv_std, batch_size=8, val_split=0.2):
    """
    Create train and validation data loaders with proper HV transforms for Swin Transformer
    """
    # Load full dataset
    full_data = pd.read_csv(train_csv)
    
    # Stratified split
    train_indices, val_indices = train_test_split(
        range(len(full_data)), 
        test_size=val_split, 
        stratify=full_data.iloc[:, 1], 
        random_state=42
    )
    
    # Create train and val dataframes
    train_data = full_data.iloc[train_indices].reset_index(drop=True)
    val_data = full_data.iloc[val_indices].reset_index(drop=True)
    
    # Save temporary CSV files
    train_data.to_csv('temp_hv_train.csv', index=False)
    val_data.to_csv('temp_hv_val.csv', index=False)
    
    # Create transforms with HV-specific normalization
    train_transform = get_hv_swin_transforms(hv_mean, hv_std, is_training=True)
    val_transform = get_hv_swin_transforms(hv_mean, hv_std, is_training=False)
    
    # Create datasets
    train_dataset = DiabeticRetinopathyHVChannelDataset(
        'temp_hv_train.csv', img_dir, train_transform
    )
    val_dataset = DiabeticRetinopathyHVChannelDataset(
        'temp_hv_val.csv', img_dir, val_transform
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=2, 
        pin_memory=True,
        drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=2, 
        pin_memory=True
    )
    
    print(f"Train loader: {len(train_loader)} batches")
    print(f"Val loader: {len(val_loader)} batches")
    
    return train_loader, val_loader

def plot_hv_swin_training_curves(history, model_name="hv_swin_model"):
    """
    Plot training curves for HV Swin Transformer
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    epochs = range(1, len(history['train_losses']) + 1)
    
    # Loss curves
    axes[0, 0].plot(epochs, history['train_losses'], 'b-', label='Training Loss', linewidth=2)
    axes[0, 0].plot(epochs, history['val_losses'], 'r-', label='Validation Loss', linewidth=2)
    axes[0, 0].set_title(f'{model_name.upper()} - Training and Validation Loss')
    axes[0, 0].set_xlabel('Epochs')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # QWK score
    axes[0, 1].plot(epochs, history['val_kappas'], 'g-', label='Validation QWK', linewidth=2)
    axes[0, 1].axhline(y=0.889, color='black', linestyle='--', alpha=0.7, label='Target QWK (0.889)', linewidth=2)
    axes[0, 1].set_title(f'{model_name.upper()} - Validation QWK Score')
    axes[0, 1].set_xlabel('Epochs')
    axes[0, 1].set_ylabel('QWK Score')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Accuracy
    axes[1, 0].plot(epochs, history['val_accuracies'], 'purple', label='Validation Accuracy', linewidth=2)
    axes[1, 0].set_title(f'{model_name.upper()} - Validation Accuracy')
    axes[1, 0].set_xlabel('Epochs')
    axes[1, 0].set_ylabel('Accuracy (%)')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # Learning rate
    axes[1, 1].plot(epochs, history['learning_rates'], 'orange', label='Learning Rate', linewidth=2)
    axes[1, 1].set_title(f'{model_name.upper()} - Learning Rate Schedule')
    axes[1, 1].set_xlabel('Epochs')
    axes[1, 1].set_ylabel('Learning Rate')
    axes[1, 1].set_yscale('log')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{model_name}_training_curves.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"Training curves saved as '{model_name}_training_curves.png'")

def validate_hv_swin_with_confusion_matrix(model, val_loader):
    """
    Generate detailed validation metrics including confusion matrix
    """
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in tqdm(val_loader, desc="Generating HV Swin validation metrics"):
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # Calculate metrics
    qwk = quadratic_weighted_kappa(all_targets, all_preds)
    cm = confusion_matrix(all_targets, all_preds)
    
    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'],
                yticklabels=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'])
    plt.title(f'HV Swin Transformer Model Confusion Matrix (QWK: {qwk:.4f})')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('hv_swin_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Classification report
    class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative']
    report = classification_report(all_targets, all_preds, target_names=class_names)
    print("\nClassification Report:")
    print(report)
    
    return qwk, cm, report

def main():
    """
    Main training function for HV Swin Transformer model
    """
    # Configuration
    config = {
        'train_csv': '/kaggle/input/aptos2019-blindness-detection/train.csv',
        'test_csv': '/kaggle/input/aptos2019-blindness-detection/test.csv',
        'train_img_dir': '/kaggle/input/aptos2019-blindness-detection/train_images',
        'test_img_dir': '/kaggle/input/aptos2019-blindness-detection/test_images',
        'batch_size': 8,  # Smaller batch size for transformer
        'num_epochs': 50,
        'learning_rate': 1e-4,
        'num_classes': 5,
        'dropout_rate': 0.5,
        'model_name': 'swin_base_patch4_window7_224'  # Swin-B model
    }
    
    print("="*70)
    print("DIABETIC RETINOPATHY HV (HUE-VALUE) CHANNEL SWIN TRANSFORMER MODEL")
    print("="*70)
    
    # Calculate HV normalization statistics
    hv_mean, hv_std = calculate_hv_normalization_stats(
        config['train_csv'], 
        config['train_img_dir'], 
        sample_size=500
    )
    
    print(f"Using HV normalization: mean={hv_mean}, std={hv_std}")
    
    # Create data loaders
    train_loader, val_loader = create_hv_data_loaders(
        config['train_csv'],
        config['train_img_dir'],
        hv_mean,
        hv_std,
        config['batch_size'],
        val_split=0.2
    )
    
    # Initialize Swin Transformer model
    model = HVSwinTransformerClassifier(
        num_classes=config['num_classes'],
        model_name=config['model_name'],
        dropout_rate=config['dropout_rate']
    )
    model.to(device)
    
    # Print model information
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    
    # Train model
    model, history = train_hv_swin_model(
        model, 
        train_loader, 
        val_loader,
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate'],
        model_name="hv_swin_model"
    )
    
    # Plot results
    plot_hv_swin_training_curves(history, "hv_swin_model")
    
    # Detailed validation analysis
    print("\nPerforming detailed validation analysis...")
    final_qwk, cm, report = validate_hv_swin_with_confusion_matrix(model, val_loader)
    
    # Save final results
    results = {
        'model_type': 'HV Channel (Hue-Value) with Swin Transformer',
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'best_qwk': float(history['best_kappa']),
        'final_validation_qwk': float(final_qwk),
        'total_epochs': int(history['total_epochs']),
        'final_val_acc': float(history['val_accuracies'][-1]),
        'hv_normalization': {'mean': hv_mean, 'std': hv_std},
        'model_variant': config['model_name'],
        'config': config,
        'device': str(device),
        'target_achieved': bool(history['best_kappa'] > 0.889),
        'transformer_advantages': [
            'Window-based self-attention for efficient computation',
            'Hierarchical architecture captures multi-scale features',
            'Shifted window approach for cross-window connections',
            'State-of-the-art performance on various vision tasks'
        ]
    }
    
    with open('hv_swin_model_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n{'='*70}")
    print("HV SWIN TRANSFORMER MODEL TRAINING COMPLETED")
    print(f"{'='*70}")
    print(f"Best QWK Score: {history['best_kappa']:.4f}")
    print(f"Final Validation Accuracy: {history['val_accuracies'][-1]:.2f}%")
    print(f"Training Epochs: {history['total_epochs']}")
    print(f"Target Achieved: {'‚úÖ Yes' if history['best_kappa'] > 0.889 else '‚ùå No'}")
    print(f"Model saved as: 'best_hv_swin_model.pth'")
    print(f"Results saved as: 'hv_swin_model_training_results.json'")
    print(f"HV Normalization used: mean={hv_mean}, std={hv_std}")
    
    # Print Swin Transformer advantages
    print("\nSwin Transformer Advantages:")
    for advantage in results['transformer_advantages']:
        print(f"  ‚Ä¢ {advantage}")
    
    # Clean up temporary files
    try:
        os.remove('temp_hv_train.csv')
        os.remove('temp_hv_val.csv')
    except:
        pass
    
    return model, history, results, hv_mean, hv_std

if __name__ == "__main__":
    model, history, results, hv_mean, hv_std = main()
    print("\nüéâ HV Channel Swin Transformer Model Training Completed Successfully!")
    
    # Save normalization stats for inference
    norm_stats = {'hv_mean': hv_mean, 'hv_std': hv_std}
    with open('hv_swin_normalization_stats.json', 'w') as f:
        json.dump(norm_stats, f, indent=2)
    print("HV normalization statistics saved to 'hv_swin_normalization_stats.json'")
