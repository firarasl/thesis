import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from tqdm import tqdm
import warnings
import random
warnings.filterwarnings('ignore')
# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Install timm if not available
try:
    import timm
    print("timm is already installed")
except ImportError:
    print("Installing timm...")
    import subprocess
    subprocess.run(['pip', 'install', 'timm'])
    import timm

def crop_black_background(image, threshold=10):
    """
    Crop black background from retinal images
    """
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    else:
        gray = image
    
    mask = gray > threshold
    coords = np.argwhere(mask)
    if len(coords) == 0:
        return image
    
    y0, x0 = coords.min(axis=0)
    y1, x1 = coords.max(axis=0) + 1
    
    cropped = image[y0:y1, x0:x1]
    return cropped

class DiabeticRetinopathyRGBHSVDataset(Dataset):
    """
    Dataset for Diabetic Retinopathy with RGB + HSV fusion - Test version
    """
    def __init__(self, csv_file, img_dir, transform=None, is_test=True, image_size=224):
        """
        Args:
            csv_file (string): Path to csv with image names
            img_dir (string): Directory with all images
            transform (callable, optional): Transform to be applied on images
            is_test (bool): Whether this is test dataset (no labels)
            image_size (int): Target image size (224x224)
        """
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        self.is_test = is_test
        self.image_size = image_size
        
        print(f"Test dataset loaded: {len(self.data)} images")
        
    def __len__(self):
        return len(self.data)
    
    def preprocess_image(self, image_path):
        """
        Preprocessing with black background cropping
        """
        # Load image
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Could not load image: {image_path}")
        
        # Convert BGR to RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Crop black background
        image = crop_black_background(image, threshold=10)
        
        # Resize to target size
        image = cv2.resize(image, (self.image_size, self.image_size))
        
        return image
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name
        if 'id_code' in self.data.columns:
            img_name = self.data.iloc[idx]['id_code']
        else:
            img_name = self.data.iloc[idx, 0]
        
        # Try different extensions
        img_extensions = ['.png', '.jpg', '.jpeg']
        img_path = None
        
        for ext in img_extensions:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension")
        
        # Load and preprocess RGB image
        rgb_image = self.preprocess_image(img_path)
        
        # Convert to HSV
        hsv_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)
        
        # Convert to PIL for transforms
        rgb_pil = Image.fromarray(rgb_image)
        hsv_pil = Image.fromarray(hsv_image)
        
        if self.transform:
            rgb_tensor = self.transform(rgb_pil)
            hsv_tensor = self.transform(hsv_pil)
        else:
            # Default transforms if none provided
            transform = transforms.Compose([
                transforms.ToTensor(),
            ])
            rgb_tensor = transform(rgb_pil)
            hsv_tensor = transform(hsv_pil)
        
        # Apply proper normalization (matches training)
        rgb_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        hsv_normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        
        rgb_tensor = rgb_normalize(rgb_tensor)
        hsv_tensor = hsv_normalize(hsv_tensor)
        
        return (rgb_tensor, hsv_tensor), img_name

class DualStreamSwinFusion(nn.Module):
    def __init__(self, num_classes=5, dropout_rate=0.3, model_name='swin_base_patch4_window7_224', pretrained=False):
        super(DualStreamSwinFusion, self).__init__()
        
        # RGB stream - set pretrained=False to avoid downloading
        self.rgb_backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0)
        self.rgb_features = self.rgb_backbone.num_features
        
        # HSV stream - set pretrained=False to avoid downloading
        self.hsv_backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0)
        self.hsv_features = self.hsv_backbone.num_features
        
        # Total features after fusion
        self.total_features = self.rgb_features + self.hsv_features
        
        # Feature fusion with attention
        self.attention = nn.Sequential(
            nn.Linear(self.total_features, 512),
            nn.ReLU(),
            nn.Linear(512, 2),
            nn.Softmax(dim=1)
        )
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.rgb_features),
            nn.Dropout(dropout_rate),
            nn.Linear(self.rgb_features, 512),
            nn.GELU(),
            nn.Dropout(dropout_rate/2),
            nn.Linear(512, num_classes)
        )
        
    def forward(self, x):
        rgb, hsv = x
        
        rgb_features = self.rgb_backbone(rgb)
        hsv_features = self.hsv_backbone(hsv)
        
        # Concatenate features for attention
        combined = torch.cat([rgb_features, hsv_features], dim=1)
        
        # Attention-based fusion
        attention_weights = self.attention(combined)
        rgb_weight, hsv_weight = attention_weights[:, 0:1], attention_weights[:, 1:2]
        
        # Apply attention weights and sum
        weighted_features = (rgb_weight * rgb_features) + (hsv_weight * hsv_features)
        
        return self.classifier(weighted_features), attention_weights

def load_model(model_path, num_classes=5, dropout_rate=0.3, model_name='swin_base_patch4_window7_224'):
    """
    Load the trained dual-stream Swin Transformer model
    """
    print(f"Loading model from: {model_path}")
    
    # Initialize the model with pretrained=False to avoid download
    model = DualStreamSwinFusion(
        num_classes=num_classes,
        dropout_rate=dropout_rate,
        model_name=model_name,
        pretrained=False  # CRITICAL: Don't try to download pretrained weights
    )
    
    # Load state dict
    try:
        # First try with weights_only=True (safe mode)
        if torch.cuda.is_available():
            checkpoint = torch.load(model_path, weights_only=True)
        else:
            checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)
    except:
        # If that fails, try with weights_only=False
        print("Weights-only loading failed, trying with weights_only=False")
        if torch.cuda.is_available():
            checkpoint = torch.load(model_path, weights_only=False)
        else:
            checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    
    # Handle different checkpoint formats
    if isinstance(checkpoint, dict):
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print("Loaded from checkpoint with 'model_state_dict' key")
        elif 'state_dict' in checkpoint:
            model.load_state_dict(checkpoint['state_dict'])
            print("Loaded from checkpoint with 'state_dict' key")
        else:
            # Try direct loading
            try:
                model.load_state_dict(checkpoint)
                print("Loaded direct state dict")
            except:
                # If all else fails, try loading with strict=False
                model.load_state_dict(checkpoint, strict=False)
                print("Loaded state dict with strict=False (some weights may not match)")
    else:
        # Assume it's a direct state dict
        try:
            model.load_state_dict(checkpoint)
            print("Loaded direct state dict")
        except:
            model.load_state_dict(checkpoint, strict=False)
            print("Loaded state dict with strict=False (some weights may not match)")
    
    model.to(device)
    model.eval()
    
    print("Dual-Stream Swin Transformer model loaded successfully!")
    return model

def perform_inference(model, test_loader, use_tta=True):
    """
    Perform inference on test dataset with optional Test Time Augmentation
    """
    model.eval()
    predictions = []
    image_names = []
    all_attention_weights = []
    
    print("Performing inference...")
    
    with torch.no_grad():
        for data, img_names in tqdm(test_loader, desc="Inference"):
            rgb_data, hsv_data = data
            rgb_data = rgb_data.to(device)
            hsv_data = hsv_data.to(device)
            
            if use_tta:
                # Test Time Augmentation
                tta_predictions = []
                tta_attention_weights = []
                
                # Original
                output, attention_weights = model((rgb_data, hsv_data))
                tta_predictions.append(torch.softmax(output, dim=1))
                tta_attention_weights.append(attention_weights)
                
                # Horizontal flip
                rgb_flipped = torch.flip(rgb_data, dims=[3])
                hsv_flipped = torch.flip(hsv_data, dims=[3])
                output_flipped, attention_flipped = model((rgb_flipped, hsv_flipped))
                tta_predictions.append(torch.softmax(output_flipped, dim=1))
                tta_attention_weights.append(attention_flipped)
                
                # Vertical flip
                rgb_vflipped = torch.flip(rgb_data, dims=[2])
                hsv_vflipped = torch.flip(hsv_data, dims=[2])
                output_vflipped, attention_vflipped = model((rgb_vflipped, hsv_vflipped))
                tta_predictions.append(torch.softmax(output_vflipped, dim=1))
                tta_attention_weights.append(attention_vflipped)
                
                # Average predictions
                avg_prediction = torch.mean(torch.stack(tta_predictions), dim=0)
                avg_attention = torch.mean(torch.stack(tta_attention_weights), dim=0)
                pred_classes = avg_prediction.argmax(dim=1)
                
                all_attention_weights.extend(avg_attention.cpu().numpy())
                
            else:
                # No TTA - single prediction
                output, attention_weights = model((rgb_data, hsv_data))
                pred_classes = output.argmax(dim=1)
                all_attention_weights.extend(attention_weights.cpu().numpy())
            
            predictions.extend(pred_classes.cpu().numpy())
            image_names.extend(img_names)
    
    # Print average attention weights for debugging
    if all_attention_weights:
        avg_attention = np.mean(all_attention_weights, axis=0)
        print(f"Average attention weights: RGB={avg_attention[0]:.3f}, HSV={avg_attention[1]:.3f}")
    
    return predictions, image_names

def create_submission(predictions, image_names, output_file='submission.csv'):
    """
    Create submission CSV file
    """
    print(f"Creating submission file: {output_file}")
    
    # Create submission dataframe
    submission_df = pd.DataFrame({
        'id_code': image_names,
        'diagnosis': predictions
    })
    
    # Save submission
    submission_df.to_csv(output_file, index=False)
    
    print(f"Submission file created with {len(submission_df)} predictions")
    print(f"Prediction distribution:")
    print(submission_df['diagnosis'].value_counts().sort_index())
    
    return submission_df

def main():
    """
    Main inference pipeline for Dual-Stream Swin Transformer
    """
    # Configuration (matches training parameters)
    config = {
        'model_path': '/kaggle/input/rgb-hsv-final-2-with-swin-transformer/best_dual_stream_model.pth',  # Your trained model
        'test_csv': '/kaggle/input/aptos2019-blindness-detection/test.csv',
        'test_img_dir': '/kaggle/input/aptos2019-blindness-detection/test_images',
        'batch_size': 8,
        'num_classes': 5,
        'dropout_rate': 0.3,
        'image_size': 224,
        'model_name': 'swin_base_patch4_window7_224',
        'use_tta': True,
        'output_file': 'submission.csv'
    }
    
    print("="*80)
    print("DUAL-STREAM SWIN TRANSFORMER RGB+HSV INFERENCE")
    print("="*80)
    print(f"Model path: {config['model_path']}")
    print(f"Model architecture: {config['model_name']}")
    print(f"Test CSV: {config['test_csv']}")
    print(f"Test images: {config['test_img_dir']}")
    print(f"Batch size: {config['batch_size']}")
    print(f"Image size: {config['image_size']}x{config['image_size']}")
    print(f"TTA enabled: {config['use_tta']}")
    print("="*80)
    
    # Load model
    model = load_model(
        config['model_path'],
        num_classes=config['num_classes'],
        dropout_rate=config['dropout_rate'],
        model_name=config['model_name']
    )
    
    # Create test dataset
    print("\nLoading test dataset...")
    test_dataset = DiabeticRetinopathyRGBHSVDataset(
        csv_file=config['test_csv'],
        img_dir=config['test_img_dir'],
        transform=None,
        is_test=True,
        image_size=config['image_size']
    )
    
    # Create test data loader
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )
    
    # Perform inference
    print("\nStarting inference with Dual-Stream Swin Transformer...")
    predictions, image_names = perform_inference(
        model, 
        test_loader, 
        use_tta=config['use_tta']
    )
    
    # Create submission file
    submission_df = create_submission(
        predictions, 
        image_names, 
        config['output_file']
    )
    
    print(f"\nInference completed successfully!")
    print(f"Submission file saved: {config['output_file']}")
    
    # Display first few predictions
    print("\nFirst 10 predictions:")
    print(submission_df.head(10))
    
    return submission_df

if __name__ == "__main__":
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # Run inference
    submission_df = main()
