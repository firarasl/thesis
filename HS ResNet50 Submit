import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class DiabeticRetinopathyHSChannelDataset(Dataset):
    """
    Custom Dataset for Diabetic Retinopathy with HS (Hue-Saturation) channels only
    """
    def __init__(self, csv_file, img_dir, transform=None, is_test=False):
        """
        Args:
            csv_file (string): Path to csv with image names and labels
            img_dir (string): Directory with all images
            transform (callable, optional): Transform to be applied on images
            is_test (bool): Whether this is test dataset (no labels)
        """
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        self.is_test = is_test
        
        print(f"Dataset loaded: {len(self.data)} images")
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name
        img_name = self.data.iloc[idx, 0]
        
        # Try different extensions
        img_extensions = ['.jpeg', '.jpg', '.png']
        img_path = None
        
        for ext in img_extensions:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension")
        
        # Load image
        image = cv2.imread(img_path)
        if image is None:
            raise ValueError(f"Could not load image: {img_path}")
        
        # Convert BGR to RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Convert RGB to HSV
        image_hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)
        
        # Extract Hue and Saturation channels (H and S)
        image_hs = image_hsv[:, :, [0, 1]]  # H, S channels
        
        # Convert to PIL Image for transforms
        image_pil = Image.fromarray(image_hs.astype('uint8'))
        
        if self.transform:
            image_pil = self.transform(image_pil)
            
        if not self.is_test:
            label = int(self.data.iloc[idx, 1])
            return image_pil, label
        else:
            return image_pil, img_name

class DRHSChannelClassifier(nn.Module):
    """
    Diabetic Retinopathy Classifier for HS (Hue-Saturation) channels
    """
    def __init__(self, num_classes=5, pretrained=False, dropout_rate=0.5):
        super(DRHSChannelClassifier, self).__init__()
        
        # Use ResNet50 as backbone
        self.backbone = models.resnet50(pretrained=pretrained)
        
        # Modify first conv layer for 2 input channels (HS)
        original_conv1 = self.backbone.conv1
        
        # Create new conv1 layer with 2 input channels
        self.backbone.conv1 = nn.Conv2d(
            2, 64, kernel_size=7, stride=2, padding=3, bias=False
        )
        
        # Initialize new conv1 weights
        if pretrained:
            with torch.no_grad():
                # Average pretrained RGB weights for HS channels initialization
                rgb_weights = original_conv1.weight
                avg_weights = rgb_weights.mean(dim=1, keepdim=True)  # Average across RGB channels
                # Initialize with averaged weights repeated for both HS channels
                self.backbone.conv1.weight = nn.Parameter(
                    torch.cat([avg_weights, avg_weights], dim=1)
                )
        else:
            # Initialize with He initialization
            nn.init.kaiming_normal_(self.backbone.conv1.weight, mode='fan_out', nonlinearity='relu')
        
        # Store original fc layer input features
        num_features = self.backbone.fc.in_features
        
        # Replace final layer with custom classifier
        self.backbone.fc = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(num_features, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate/2),
            nn.Linear(512, num_classes)
        )
        
        # Initialize weights for new layers
        for m in self.backbone.fc.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        return self.backbone(x)

def get_inference_transforms():
    """
    Define inference transforms for HS (Hue-Saturation) channels
    """
    # Two-channel normalization for HS channels (same as training)
    mean = [0.5, 0.5]  # H, S channel means (normalized)
    std = [0.3, 0.3]   # H, S channel stds (normalized)
    
    inference_transforms = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])
    
    return inference_transforms

def load_trained_model(model_path, num_classes=5, dropout_rate=0.5):
    """
    Load the trained HS channel model
    """
    print(f"Loading model from: {model_path}")
    
    # Initialize model architecture (same as training)
    model = DRHSChannelClassifier(
        num_classes=num_classes,
        pretrained=False,  # We're loading trained weights
        dropout_rate=dropout_rate
    )
    
    # Load trained weights
    if torch.cuda.is_available():
        checkpoint = torch.load(model_path)
    else:
        checkpoint = torch.load(model_path, map_location='cpu')
    
    model.load_state_dict(checkpoint)
    model.to(device)
    model.eval()  # Set to evaluation mode
    
    print("Model loaded successfully!")
    return model

def perform_inference(model, test_loader, use_tta=True):
    """
    Perform inference on test dataset
    Args:
        model: Trained HS channel model
        test_loader: DataLoader for test dataset
        use_tta: Whether to use Test Time Augmentation
    """
    model.eval()
    predictions = []
    image_names = []
    prediction_probs = []
    
    print(f"Performing inference on {len(test_loader.dataset)} test images...")
    print(f"Test Time Augmentation: {'Enabled' if use_tta else 'Disabled'}")
    
    with torch.no_grad():
        for batch_idx, (data, names) in enumerate(tqdm(test_loader, desc="Inference")):
            data = data.to(device)
            
            if use_tta:
                # Test Time Augmentation - multiple predictions per image
                batch_preds = []
                
                # Original image
                outputs = model(data)
                batch_preds.append(torch.softmax(outputs, dim=1))
                
                # Horizontal flip
                outputs_hflip = model(torch.flip(data, dims=[3]))
                batch_preds.append(torch.softmax(outputs_hflip, dim=1))
                
                # Vertical flip
                outputs_vflip = model(torch.flip(data, dims=[2]))
                batch_preds.append(torch.softmax(outputs_vflip, dim=1))
                
                # Average all predictions
                avg_probs = torch.mean(torch.stack(batch_preds), dim=0)
                preds = avg_probs.argmax(dim=1)
                
            else:
                # Single prediction
                outputs = model(data)
                avg_probs = torch.softmax(outputs, dim=1)
                preds = avg_probs.argmax(dim=1)
            
            predictions.extend(preds.cpu().numpy())
            prediction_probs.extend(avg_probs.cpu().numpy())
            image_names.extend(names)
    
    return predictions, prediction_probs, image_names

def create_submission_file(predictions, image_names, filename='submission.csv'):
    """
    Create submission file in the required format
    """
    # Create submission DataFrame
    submission_df = pd.DataFrame({
        'id_code': image_names,
        'diagnosis': predictions
    })
    
    # Sort by id_code for consistency
    submission_df = submission_df.sort_values('id_code').reset_index(drop=True)
    
    # Save to CSV
    submission_df.to_csv(filename, index=False)
    
    print(f"\nðŸ“„ Submission file created: {filename}")
    print(f"Total predictions: {len(submission_df)}")
    
    # Display prediction distribution
    print("\nðŸ“Š Prediction Distribution:")
    pred_counts = submission_df['diagnosis'].value_counts().sort_index()
    for class_idx, count in pred_counts.items():
        percentage = (count / len(submission_df)) * 100
        print(f"  Class {class_idx}: {count:4d} ({percentage:5.1f}%)")
    
    return submission_df

def save_detailed_predictions(predictions, prediction_probs, image_names, filename='detailed_predictions.csv'):
    """
    Save detailed predictions with probabilities for each class
    """
    # Create detailed DataFrame
    detailed_df = pd.DataFrame({
        'id_code': image_names,
        'predicted_class': predictions,
        'prob_0': [probs[0] for probs in prediction_probs],
        'prob_1': [probs[1] for probs in prediction_probs],
        'prob_2': [probs[2] for probs in prediction_probs],
        'prob_3': [probs[3] for probs in prediction_probs],
        'prob_4': [probs[4] for probs in prediction_probs],
        'max_probability': [max(probs) for probs in prediction_probs]
    })
    
    # Sort by id_code
    detailed_df = detailed_df.sort_values('id_code').reset_index(drop=True)
    
    # Save to CSV
    detailed_df.to_csv(filename, index=False)
    print(f"ðŸ“„ Detailed predictions saved: {filename}")
    
    # Show confidence statistics
    print(f"\nðŸ“ˆ Confidence Statistics:")
    print(f"  Mean confidence: {detailed_df['max_probability'].mean():.3f}")
    print(f"  Min confidence:  {detailed_df['max_probability'].min():.3f}")
    print(f"  Max confidence:  {detailed_df['max_probability'].max():.3f}")
    
    return detailed_df

def main():
    """
    Main inference function
    """
    # Configuration - UPDATE THESE PATHS ACCORDING TO YOUR SETUP
    config = {
        'model_path': '/kaggle/input/diabetic-retinopathy-hs-channel-model/best_hs_model.pth',  # Path to your trained model
        'test_csv': '/kaggle/input/aptos2019-blindness-detection/test.csv',  # Update path
        'test_img_dir': '/kaggle/input/aptos2019-blindness-detection/test_images',  # Update path
        'batch_size': 16,
        'num_classes': 5,
        'dropout_rate': 0.5,
        'use_tta': True,  # Test Time Augmentation
        'submission_filename': 'submission.csv'
    }
    
    print("="*70)
    print("DIABETIC RETINOPATHY HS MODEL INFERENCE")
    print("="*70)
    
    # Check if model file exists
    if not os.path.exists(config['model_path']):
        print(f" Error: Model file '{config['model_path']}' not found!")
        print("Please make sure you have the trained model file in the current directory.")
        return
    
    # Check if test data exists
    if not os.path.exists(config['test_csv']):
        print(f" Error: Test CSV file '{config['test_csv']}' not found!")
        print("Please update the test_csv path in the config.")
        return
    
    if not os.path.exists(config['test_img_dir']):
        print(f" Error: Test images directory '{config['test_img_dir']}' not found!")
        print("Please update the test_img_dir path in the config.")
        return
    
    # Load trained model
    model = load_trained_model(
        config['model_path'], 
        config['num_classes'], 
        config['dropout_rate']
    )
    
    # Get inference transforms
    inference_transforms = get_inference_transforms()
    
    # Load test dataset
    test_dataset = DiabeticRetinopathyHSChannelDataset(
        csv_file=config['test_csv'],
        img_dir=config['test_img_dir'],
        transform=inference_transforms,
        is_test=True
    )
    
    # Create test data loader
    test_loader = DataLoader(
        test_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False,
        num_workers=4, 
        pin_memory=True
    )
    
    # Perform inference
    predictions, prediction_probs, image_names = perform_inference(
        model, test_loader, use_tta=config['use_tta']
    )
    
    # Create submission file
    submission_df = create_submission_file(
        predictions, image_names, config['submission_filename']
    )
    
    # Save detailed predictions
    detailed_df = save_detailed_predictions(
        predictions, prediction_probs, image_names
    )
    
    print(f"\n{'='*70}")
    print(" INFERENCE COMPLETED SUCCESSFULLY!")
    print(f"{'='*70}")
    print(f" Files created:")
    print(f"  - {config['submission_filename']} (for submission)")
    print(f"  - detailed_predictions.csv (with probabilities)")
    print(f"\n Ready for submission!")
    
    return submission_df, detailed_df, predictions

# Run inference
if __name__ == "__main__":
    # Update the config paths in main() function before running
    submission_df, detailed_df, predictions = main()
    print("\nðŸŽ‰ Inference completed successfully!")
