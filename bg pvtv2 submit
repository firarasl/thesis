import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from tqdm import tqdm
import warnings
import json
warnings.filterwarnings('ignore')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Import timm for PVTv2
import timm

class DiabeticRetinopathyGBDataset(Dataset):
    """
    Custom Dataset for Diabetic Retinopathy with GB (Green-Blue) channels (Test Dataset)
    """
    def __init__(self, csv_file, img_dir, transform=None):
        """
        Args:
            csv_file (string): Path to csv with image names
            img_dir (string): Directory with all images
            transform (callable, optional): Transform to be applied on images
        """
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        
        print(f"Test dataset loaded: {len(self.data)} images")
        print(f"Input channels: 2 (Green + Blue)")
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name (assuming the CSV has 'id_code' column)
        img_name = str(self.data.iloc[idx, 0])  # First column should be image ID
        
        # Try different extensions
        img_extensions = ['.png', '.jpg', '.jpeg']
        img_path = None
        
        for ext in img_extensions:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension")
        
        try:
            # Load image
            image = cv2.imread(img_path)
            if image is None:
                raise ValueError(f"Could not load image: {img_path}")
            
            # Convert BGR to RGB
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Extract Green and Blue channels only
            image_gb = np.zeros((image_rgb.shape[0], image_rgb.shape[1], 2), dtype=np.uint8)
            image_gb[:, :, 0] = image_rgb[:, :, 1]  # Green channel
            image_gb[:, :, 1] = image_rgb[:, :, 2]  # Blue channel
            
            # Convert to tensor format directly
            image_tensor = torch.from_numpy(image_gb).permute(2, 0, 1).float() / 255.0
            
            if self.transform:
                # Apply transforms that work with tensors
                image_tensor = self.transform(image_tensor)
                
        except Exception as e:
            print(f"Error loading image {img_name}: {e}")
            # Return a dummy tensor in case of error
            image_tensor = torch.zeros((2, 512, 512), dtype=torch.float32)
            
        return image_tensor, img_name

class DRGBChannelPVT2Classifier(nn.Module):
    """
    Diabetic Retinopathy Classifier for GB (Green-Blue) channels using PVT2 transformer
    """
    def __init__(self, num_classes=5, pretrained=False, dropout_rate=0.5, model_name='pvt_v2_b2'):
        super(DRGBChannelPVT2Classifier, self).__init__()
        
        # Use PVT2 from timm as backbone
        self.backbone = timm.create_model(
            model_name, 
            pretrained=pretrained, 
            num_classes=0,  # We'll add our own classifier
            in_chans=2  # GB has 2 channels (Green, Blue)
        )
        
        # Get the number of features from the backbone
        self.num_features = self.backbone.num_features
        
        # Replace the classifier head
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(self.num_features, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate/2),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate/4),
            nn.Linear(256, num_classes)
        )
        
    def forward(self, x):
        # Forward through PVT2 backbone
        features = self.backbone(x)  # Shape: (batch_size, num_features)
        
        # Classify
        output = self.classifier(features)
        return output

class GBTestTransforms:
    """
    Custom transforms for GB (2-channel) test images
    """
    def __init__(self, size=(512, 512), mean=None, std=None):
        self.size = size
        self.mean = mean or [0.5, 0.5]
        self.std = std or [0.5, 0.5]
        
    def __call__(self, tensor):
        # tensor shape: (2, H, W)
        
        # Resize
        tensor = torch.nn.functional.interpolate(
            tensor.unsqueeze(0), 
            size=self.size, 
            mode='bilinear', 
            align_corners=False
        ).squeeze(0)
        
        # Normalize - CRITICAL: Apply channel-specific normalization
        for i in range(2):
            tensor[i] = (tensor[i] - self.mean[i]) / self.std[i]
            
        return tensor

def get_gb_test_transforms(gb_mean, gb_std):
    """
    Define preprocessing transforms for test images (no augmentation)
    Uses GB-specific normalization from training
    """
    test_transforms = GBTestTransforms(
        size=(512, 512), 
        mean=gb_mean, 
        std=gb_std
    )
    
    return test_transforms

def load_normalization_stats(stats_path='gb_pvt2_normalization_stats.json'):
    """
    Load GB normalization statistics from training
    """
    try:
        with open(stats_path, 'r') as f:
            stats = json.load(f)
        print(f"Loaded normalization stats: mean={stats['gb_mean']}, std={stats['gb_std']}")
        return stats['gb_mean'], stats['gb_std']
    except FileNotFoundError:
        print(f"Warning: Normalization stats file {stats_path} not found. Using default values.")
        return [0.5, 0.5], [0.5, 0.5]

def safe_torch_load(model_path, device):
    """
    Safely load torch model with proper handling of weights_only parameter
    """
    print(f"Loading model from: {model_path}")
    
    try:
        # First try with weights_only=True (safer)
        checkpoint = torch.load(model_path, map_location=device, weights_only=True)
        print("Model loaded with weights_only=True")
        return checkpoint
    except Exception as e:
        print(f"Loading with weights_only=True failed: {e}")
        print("Trying with weights_only=False (requires trust in the model source)...")
        
        # If that fails, try with weights_only=False
        try:
            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
            print("Model loaded with weights_only=False")
            return checkpoint
        except Exception as e2:
            print(f"Loading with weights_only=False also failed: {e2}")
            raise

def load_trained_model(model_path, num_classes=5, dropout_rate=0.5):
    """
    Load the pre-trained GB Channel PVT2 model
    """
    # Initialize model architecture WITHOUT downloading pretrained weights
    model = DRGBChannelPVT2Classifier(
        num_classes=num_classes,
        dropout_rate=dropout_rate,
        pretrained=False,
        model_name='pvt_v2_b2'
    )
    
    # Load trained weights safely
    checkpoint = safe_torch_load(model_path, device)
    
    # Handle different checkpoint formats
    if isinstance(checkpoint, dict):
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print("Loaded from 'model_state_dict'")
        elif 'state_dict' in checkpoint:
            model.load_state_dict(checkpoint['state_dict'])
            print("Loaded from 'state_dict'")
        else:
            # Assume it's the state dict directly
            model.load_state_dict(checkpoint)
            print("Loaded from direct state dict")
    else:
        model.load_state_dict(checkpoint)
        print("Loaded from direct state dict")
        
    model.to(device)
    model.eval()
    
    print("GB Channel PVT2 Model loaded successfully!")
    
    # Print model info
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Model parameters: {total_params:,}")
    
    return model

def generate_predictions(model, test_loader, use_tta=False):
    """
    Generate predictions for test set
    
    Args:
        model: Trained model
        test_loader: DataLoader for test set
        use_tta: Whether to use Test Time Augmentation (TTA)
    """
    model.eval()
    predictions = []
    image_names = []
    all_probabilities = []
    
    print("Generating predictions...")
    
    with torch.no_grad():
        for data, names in tqdm(test_loader, desc="Processing test images"):
            data = data.to(device)
            
            if use_tta:
                # Test Time Augmentation for GB channels
                outputs = model(data)
                
                # Horizontal flip
                outputs += model(torch.flip(data, dims=[3]))
                
                # Vertical flip
                outputs += model(torch.flip(data, dims=[2]))
                
                # Average the predictions
                outputs = outputs / 3.0
            else:
                outputs = model(data)
            
            # Convert logits to probabilities
            probs = torch.softmax(outputs, dim=1)
            
            # Get predicted classes
            preds = probs.argmax(dim=1)
            
            predictions.extend(preds.cpu().numpy())
            image_names.extend(names)
            all_probabilities.extend(probs.cpu().numpy())
    
    return predictions, image_names, np.array(all_probabilities)

def create_submission_file(predictions, image_names, output_file='gb_pvt2_submission.csv'):
    """
    Create submission file in Kaggle format
    """
    submission_df = pd.DataFrame({
        'id_code': image_names,
        'diagnosis': predictions
    })
    
    # Sort by id_code to ensure consistent ordering
    submission_df = submission_df.sort_values('id_code')
    
    # Save to CSV
    submission_df.to_csv(output_file, index=False)
    
    print(f"Submission file saved: {output_file}")
    print(f"Total predictions: {len(submission_df)}")
    
    # Print prediction distribution
    print("\nPrediction distribution:")
    label_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative']
    for i, label in enumerate(label_names):
        count = (submission_df['diagnosis'] == i).sum()
        percentage = count / len(submission_df) * 100
        print(f"  Class {i} ({label}): {count} images ({percentage:.1f}%)")
    
    return submission_df

def main():
    """
    Main inference pipeline for GB Channel PVT2 model
    """
    # Configuration
    config = {
        'model_path': '/kaggle/input/bg-pvt2-training/best_gb_pvt2_model.pth',  # Update with your model path
        'normalization_stats_path': '/kaggle/input/bg-pvt2-training/gb_pvt2_normalization_stats.json',  # Update path
        'test_csv': '/kaggle/input/aptos2019-blindness-detection/test.csv',
        'test_img_dir': '/kaggle/input/aptos2019-blindness-detection/test_images',
        'batch_size': 16,  # Smaller batch size for transformer inference
        'num_classes': 5,
        'dropout_rate': 0.5,
        'use_tta': True,  # Use Test Time Augmentation for better results
        'output_file': 'submission.csv'
    }
    
    print("="*70)
    print("DIABETIC RETINOPATHY GB (GREEN-BLUE) CHANNEL PVT2 MODEL - INFERENCE")
    print("="*70)
    print("Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print("="*70)
    
    # Check if test files exist
    if not os.path.exists(config['test_csv']):
        print(f"Error: Test CSV file not found at {config['test_csv']}")
        return
    
    if not os.path.exists(config['test_img_dir']):
        print(f"Error: Test images directory not found at {config['test_img_dir']}")
        return
    
    if not os.path.exists(config['model_path']):
        print(f"Error: Model file not found at {config['model_path']}")
        print("Please make sure the model file exists or update the model_path in the config.")
        return
    
    # Load normalization statistics
    gb_mean, gb_std = load_normalization_stats(config['normalization_stats_path'])
    
    # Load trained model
    model = load_trained_model(
        config['model_path'], 
        config['num_classes'], 
        config['dropout_rate']
    )
    
    # Get GB-specific test transforms
    test_transforms = get_gb_test_transforms(gb_mean, gb_std)
    
    # Load test dataset
    print("\nLoading test data...")
    test_dataset = DiabeticRetinopathyGBDataset(
        csv_file=config['test_csv'],
        img_dir=config['test_img_dir'],
        transform=test_transforms
    )
    
    # Create test data loader
    test_loader = DataLoader(
        test_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=4,
        pin_memory=True
    )
    
    # Generate predictions
    print(f"\nGenerating predictions with TTA: {config['use_tta']}")
    predictions, image_names, probabilities = generate_predictions(
        model, 
        test_loader, 
        use_tta=config['use_tta']
    )
    
    # Create submission file
    print("\nCreating submission file...")
    submission_df = create_submission_file(
        predictions, 
        image_names, 
        config['output_file']
    )
    
    # Calculate confidence statistics
    max_probs = np.max(probabilities, axis=1)
    print(f"\nConfidence statistics:")
    print(f"  Mean confidence: {np.mean(max_probs):.4f}")
    print(f"  Std confidence: {np.std(max_probs):.4f}")
    print(f"  Min confidence: {np.min(max_probs):.4f}")
    print(f"  Max confidence: {np.max(max_probs):.4f}")
    
    # Show samples with lowest confidence
    low_confidence_indices = np.argsort(max_probs)[:5]
    print(f"\n5 predictions with lowest confidence:")
    for i, idx in enumerate(low_confidence_indices):
        print(f"  {i+1}. {image_names[idx]}: Class {predictions[idx]} (confidence: {max_probs[idx]:.4f})")
    
    # Save prediction probabilities for analysis
    prob_df = pd.DataFrame(probabilities, columns=[f'class_{i}' for i in range(5)])
    prob_df['id_code'] = image_names
    prob_df['prediction'] = predictions
    prob_df['confidence'] = max_probs
    prob_df.to_csv('gb_pvt2_prediction_probabilities.csv', index=False)
    print(f"\nPrediction probabilities saved to 'gb_pvt2_prediction_probabilities.csv'")
    
    print("\n" + "="*70)
    print("GB CHANNEL PVT2 INFERENCE COMPLETED SUCCESSFULLY!")
    print("="*70)
    print(f"Submission file: {config['output_file']}")
    print(f"Total test images processed: {len(predictions)}")
    print(f"GB Normalization used: mean={gb_mean}, std={gb_std}")
    print("="*70)
    
    return submission_df

if __name__ == "__main__":
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # Run inference
    submission_df = main()
