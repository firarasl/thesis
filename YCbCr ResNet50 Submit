import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class DiabeticRetinopathyYCbCrDataset(Dataset):
    """
    Custom Dataset for Diabetic Retinopathy with YCbCr color space - Inference version
    """
    def __init__(self, csv_file, img_dir, transform=None):
        """
        Args:
            csv_file (string): Path to csv with image names
            img_dir (string): Directory with all images
            transform (callable, optional): Transform to be applied on images
        """
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        
        print(f"Test dataset loaded: {len(self.data)} images")
        
    def __len__(self):
        return len(self.data)
    
    def rgb_to_ycbcr(self, rgb_image):
        """
        Convert RGB image to YCbCr color space using OpenCV
        """
        # Convert PIL Image to numpy array if needed
        if isinstance(rgb_image, Image.Image):
            rgb_array = np.array(rgb_image)
        else:
            rgb_array = rgb_image
            
        # Convert RGB to YCrCb using OpenCV
        ycbcr_array = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2YCrCb)
        
        # Convert YCrCb to YCbCr format (swap Cr and Cb channels)
        y_channel = ycbcr_array[:, :, 0]
        cr_channel = ycbcr_array[:, :, 1]  # This is Cr
        cb_channel = ycbcr_array[:, :, 2]  # This is Cb
        
        # Rearrange to YCbCr format
        ycbcr_correct = np.stack([y_channel, cb_channel, cr_channel], axis=2)
        
        return Image.fromarray(ycbcr_correct)
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name (assuming first column is image name/id)
        img_name = self.data.iloc[idx, 0]
        
        # Try different extensions
        img_extensions = ['.jpeg', '.jpg', '.png']
        img_path = None
        
        for ext in img_extensions:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension in {self.img_dir}")
        
        # Load image in RGB format first
        image = cv2.imread(img_path)
        if image is None:
            raise ValueError(f"Could not load image: {img_path}")
        
        # Convert BGR (OpenCV default) to RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Convert to PIL Image
        image = Image.fromarray(image)
        
        # Convert RGB to YCbCr
        image = self.rgb_to_ycbcr(image)
        
        if self.transform:
            image = self.transform(image)
            
        return image, img_name

class DRYCbCrClassifier(nn.Module):
    """
    Diabetic Retinopathy Classifier using pretrained ResNet50 adapted for YCbCr images
    """
    def __init__(self, num_classes=5, pretrained=False, dropout_rate=0.5):
        super(DRYCbCrClassifier, self).__init__()
        
        # Use ResNet50 as backbone
        self.backbone = models.resnet50(pretrained=pretrained)
        
        # Modify first convolution layer to accept YCbCr (3 channels)
        if pretrained:
            original_conv1_weight = self.backbone.conv1.weight.clone()
            
            # Initialize new conv1 for YCbCr
            new_conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
            
            # Y channel: average of RGB channels (luminance is similar to grayscale)
            new_conv1.weight.data[:, 0, :, :] = original_conv1_weight.mean(dim=1)
            
            # Cb channel: initialize with blue channel weights
            new_conv1.weight.data[:, 1, :, :] = original_conv1_weight[:, 2, :, :]
            
            # Cr channel: initialize with red channel weights  
            new_conv1.weight.data[:, 2, :, :] = original_conv1_weight[:, 0, :, :]
            
            self.backbone.conv1 = new_conv1
        
        # Store original fc layer input features
        num_features = self.backbone.fc.in_features
        
        # Replace final layer with custom classifier
        self.backbone.fc = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(num_features, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate/2),
            nn.Linear(512, num_classes)
        )
        
    def forward(self, x):
        return self.backbone(x)

def get_inference_transforms():
    """
    Define preprocessing transforms for inference (no augmentation)
    """
    # YCbCr normalization values (same as used in training)
    ycbcr_mean = [0.5, 0.5, 0.5]
    ycbcr_std = [0.25, 0.25, 0.25]
    
    inference_transforms = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize(mean=ycbcr_mean, std=ycbcr_std)
    ])
    
    return inference_transforms

def load_trained_model(model_path, num_classes=5, dropout_rate=0.5):
    """
    Load the trained YCbCr model from checkpoint
    """
    print(f"Loading model from: {model_path}")
    
    # Initialize model architecture (same as training)
    model = DRYCbCrClassifier(
        num_classes=num_classes, 
        pretrained=False,  # We're loading trained weights
        dropout_rate=dropout_rate
    )
    
    # Load trained weights
    try:
        checkpoint = torch.load(model_path, map_location=device)
        model.load_state_dict(checkpoint)
        print(" Model loaded successfully!")
    except Exception as e:
        print(f"Error loading model: {e}")
        raise
    
    model.to(device)
    model.eval()  # Set to evaluation mode
    
    return model

def generate_predictions(model, test_loader, use_tta=False):
    """
    Generate predictions on test dataset
    Args:
        model: Trained model
        test_loader: DataLoader for test data
        use_tta: Whether to use Test Time Augmentation (optional)
    """
    model.eval()
    predictions = []
    image_names = []
    prediction_probabilities = []
    
    print("Generating predictions...")
    
    with torch.no_grad():
        for batch_idx, (data, names) in enumerate(tqdm(test_loader, desc="Predicting")):
            data = data.to(device)
            
            if use_tta:
                # Test Time Augmentation (optional)
                outputs_list = []
                
                # Original image
                outputs = model(data)
                outputs_list.append(outputs)
                
                # Horizontal flip
                data_hflip = torch.flip(data, dims=[3])
                outputs_hflip = model(data_hflip)
                outputs_list.append(outputs_hflip)
                
                # Vertical flip
                data_vflip = torch.flip(data, dims=[2])
                outputs_vflip = model(data_vflip)
                outputs_list.append(outputs_vflip)
                
                # Average predictions
                outputs = torch.mean(torch.stack(outputs_list), dim=0)
            else:
                # Single prediction
                outputs = model(data)
            
            # Get probabilities and predictions
            probs = torch.softmax(outputs, dim=1)
            preds = probs.argmax(dim=1)
            
            # Store results
            predictions.extend(preds.cpu().numpy())
            prediction_probabilities.extend(probs.cpu().numpy())
            image_names.extend(names)
            
            # Print progress every 100 batches
            if (batch_idx + 1) % 100 == 0:
                print(f"Processed {(batch_idx + 1) * test_loader.batch_size} images...")
    
    return predictions, image_names, prediction_probabilities

def create_submission_file(predictions, image_names, output_filename='submission.csv'):
    """
    Create submission file in the format required by Kaggle
    """
    print(f"Creating submission file: {output_filename}")
    
    # Create DataFrame
    submission_df = pd.DataFrame({
        'id_code': image_names,
        'diagnosis': predictions
    })
    
    # Verify the format
    print(f"Submission shape: {submission_df.shape}")
    print("Sample predictions:")
    print(submission_df.head(10))
    
    # Check distribution of predictions
    print("\nPrediction distribution:")
    prediction_counts = submission_df['diagnosis'].value_counts().sort_index()
    for label, count in prediction_counts.items():
        percentage = (count / len(submission_df)) * 100
        print(f"  Class {label}: {count} images ({percentage:.1f}%)")
    
    # Save to CSV
    submission_df.to_csv(output_filename, index=False)
    print(f" Submission file saved: {output_filename}")
    
    return submission_df

def save_detailed_predictions(predictions, image_names, probabilities, output_filename='detailed_predictions.csv'):
    """
    Save detailed predictions with probabilities for analysis
    """
    print(f"Saving detailed predictions: {output_filename}")
    
    # Create DataFrame with probabilities
    detailed_df = pd.DataFrame({
        'id_code': image_names,
        'prediction': predictions,
        'prob_class_0': [prob[0] for prob in probabilities],
        'prob_class_1': [prob[1] for prob in probabilities],
        'prob_class_2': [prob[2] for prob in probabilities],
        'prob_class_3': [prob[3] for prob in probabilities],
        'prob_class_4': [prob[4] for prob in probabilities],
        'max_probability': [max(prob) for prob in probabilities]
    })
    
    detailed_df.to_csv(output_filename, index=False)
    print(f" Detailed predictions saved: {output_filename}")
    
    # Print confidence statistics
    mean_confidence = detailed_df['max_probability'].mean()
    print(f"Mean prediction confidence: {mean_confidence:.4f}")
    print(f"Min confidence: {detailed_df['max_probability'].min():.4f}")
    print(f"Max confidence: {detailed_df['max_probability'].max():.4f}")
    
    return detailed_df

def main():
    """
    Main inference pipeline
    """
    # Configuration
    config = {
        'model_path': '/kaggle/input/ycbcr-dr-classification/best_ycbcr_model.pth',  # Update this path
        'test_csv': '/kaggle/input/aptos2019-blindness-detection/test.csv',  # Update this path
        'test_img_dir': '/kaggle/input/aptos2019-blindness-detection/test_images',  # Update this path
        'batch_size': 32,  # Can be larger for inference
        'num_classes': 5,
        'dropout_rate': 0.5,
        'use_tta': False,  # Set to True for Test Time Augmentation (slower but potentially better)
        'output_filename': 'submission.csv'
    }
    
    print("=" * 70)
    print("DIABETIC RETINOPATHY YCbCr MODEL - INFERENCE")
    print("=" * 70)
    print("Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print("=" * 70)
    
    # Check if model file exists
    if not os.path.exists(config['model_path']):
        print(f" Model file not found: {config['model_path']}")
        print("Please check the model path and make sure the file exists.")
        return
    
    # Check if test files exist
    if not os.path.exists(config['test_csv']):
        print(f" Test CSV file not found: {config['test_csv']}")
        return
        
    if not os.path.exists(config['test_img_dir']):
        print(f" Test images directory not found: {config['test_img_dir']}")
        return
    
    # Load model
    print("\n1. Loading trained model...")
    model = load_trained_model(
        model_path=config['model_path'],
        num_classes=config['num_classes'],
        dropout_rate=config['dropout_rate']
    )
    
    # Get inference transforms
    print("\n2. Setting up data preprocessing...")
    inference_transforms = get_inference_transforms()
    
    # Load test dataset
    print("\n3. Loading test dataset...")
    test_dataset = DiabeticRetinopathyYCbCrDataset(
        csv_file=config['test_csv'],
        img_dir=config['test_img_dir'],
        transform=inference_transforms
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,  # Don't shuffle for inference
        num_workers=4,
        pin_memory=True
    )
    
    print(f"Test dataset: {len(test_dataset)} images")
    print(f"Batch size: {config['batch_size']}")
    print(f"Number of batches: {len(test_loader)}")
    
    # Generate predictions
    print("\n4. Generating predictions...")
    if config['use_tta']:
        print("Using Test Time Augmentation (TTA) - this may take longer...")
    
    predictions, image_names, probabilities = generate_predictions(
        model=model,
        test_loader=test_loader,
        use_tta=config['use_tta']
    )
    
    print(f" Generated predictions for {len(predictions)} images")
    
    # Create submission file
    print("\n5. Creating submission file...")
    submission_df = create_submission_file(
        predictions=predictions,
        image_names=image_names,
        output_filename=config['output_filename']
    )
    
    # Save detailed predictions
    print("\n6. Saving detailed predictions...")
    detailed_df = save_detailed_predictions(
        predictions=predictions,
        image_names=image_names,
        probabilities=probabilities
    )
    
    print("\n" + "=" * 70)
    print("INFERENCE COMPLETED SUCCESSFULLY!")
    print("=" * 70)
    print(f"Files generated:")
    print(f"  • {config['output_filename']} - Kaggle submission file")
    print(f"  • detailed_predictions.csv - Detailed predictions with probabilities")
    print(f"\nTotal images processed: {len(predictions)}")
    print("=" * 70)

if __name__ == "__main__":
    main()
