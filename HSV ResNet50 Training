import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import json
from datetime import datetime
warnings.filterwarnings('ignore')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class DiabeticRetinopathyHSVDataset(Dataset):
    """
    Custom Dataset for Diabetic Retinopathy with HSV color space conversion
    Converts RGB images to HSV for training - Experiment 2.3
    """
    def __init__(self, csv_file, img_dir, transform=None, is_test=False):
        """
        Args:
            csv_file (string): Path to csv with image names and labels
            img_dir (string): Directory with all images
            transform (callable, optional): Transform to be applied on images
            is_test (bool): Whether this is test dataset (no labels)
        """
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        self.is_test = is_test
        
        # Print dataset statistics
        if not is_test:
            print(f"HSV Dataset loaded: {len(self.data)} images")
            label_counts = self.data.iloc[:, 1].value_counts().sort_index()
            print("Label distribution:")
            for label, count in label_counts.items():
                print(f"  Class {label}: {count} images ({count/len(self.data)*100:.1f}%)")
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name
        img_name = self.data.iloc[idx, 0]  # Assuming first column is image name
        
        # Try different extensions
        img_extensions = ['.jpeg', '.jpg', '.png']
        img_path = None
        
        for ext in img_extensions:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension")
        
        # Load image and convert to HSV
        image = cv2.imread(img_path)
        if image is None:
            raise ValueError(f"Could not load image: {img_path}")
        
        # Convert BGR (OpenCV default) to RGB first
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Convert RGB to HSV
        image_hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)
        
        # Convert to PIL Image for transforms
        image_hsv = Image.fromarray(image_hsv)
        
        if self.transform:
            image_hsv = self.transform(image_hsv)
            
        if not self.is_test:
            label = int(self.data.iloc[idx, 1])  # Assuming second column is label
            return image_hsv, label
        else:
            return image_hsv, img_name

class DRHSVClassifier(nn.Module):
    """
    Diabetic Retinopathy Classifier using pretrained ResNet50 adapted for HSV images
    Modified to handle 3-channel HSV input instead of RGB
    """
    def __init__(self, num_classes=5, pretrained=True, dropout_rate=0.5):
        super(DRHSVClassifier, self).__init__()
        
        # Use ResNet50 as backbone (pretrained on RGB, but we'll adapt it)
        self.backbone = models.resnet50(pretrained=pretrained)
        
        # If using pretrained weights, we need to adapt the first layer for HSV
        if pretrained:
            # Get the original first conv layer weights (trained on RGB)
            original_conv1_weights = self.backbone.conv1.weight.data.clone()
            
            # The first conv layer expects 3 channels (like RGB), so we keep it as is
            # but we'll need to be careful about normalization later
            print("Using pretrained RGB weights adapted for HSV input")
        
        # Store original fc layer input features
        num_features = self.backbone.fc.in_features
        
        # Replace final layer with custom classifier
        self.backbone.fc = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(num_features, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate/2),
            nn.Linear(512, num_classes)
        )
        
        # Initialize weights for new layers
        for m in self.backbone.fc.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        return self.backbone(x)

def get_hsv_transforms():
    """
    Define data augmentation and preprocessing transforms optimized for HSV images
    HSV color space requires different normalization than RGB
    """
    # Custom normalization for HSV color space
    # H: [0, 179] -> normalized to [0, 1] by dividing by 179
    # S: [0, 255] -> normalized to [0, 1] by dividing by 255  
    # V: [0, 255] -> normalized to [0, 1] by dividing by 255
    
    # For HSV, we use different normalization values
    # These are approximated based on HSV characteristics
    hsv_mean = [0.5, 0.5, 0.5]  # Approximate HSV means
    hsv_std = [0.5, 0.3, 0.3]   # Approximate HSV standard deviations
    
    train_transforms = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),
        # Note: ColorJitter affects RGB channels, so we'll be more conservative with HSV
        transforms.RandomResizedCrop(512, scale=(0.8, 1.0)),
        transforms.ToTensor(),  # Converts to [0,1] range
        transforms.Normalize(mean=hsv_mean, std=hsv_std)
    ])
    
    val_transforms = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize(mean=hsv_mean, std=hsv_std)
    ])
    
    return train_transforms, val_transforms

def quadratic_weighted_kappa(y_true, y_pred):
    """
    Calculate Quadratic Weighted Kappa (QWK) score
    This is the primary evaluation metric for the Kaggle competition
    """
    return cohen_kappa_score(y_true, y_pred, weights='quadratic')

def train_hsv_model(model, train_loader, val_loader, num_epochs=60, learning_rate=1e-4):
    """
    Train the HSV diabetic retinopathy model with advanced training techniques
    """
    # Loss function with class weights for imbalanced dataset
    criterion = nn.CrossEntropyLoss()
    
    # Optimizer with weight decay
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    
    # Learning rate scheduler
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )
    
    # Training history
    history = {
        'train_losses': [],
        'val_losses': [],
        'val_kappas': [],
        'val_accuracies': [],
        'learning_rates': []
    }
    
    best_kappa = 0.0
    best_model_state = None
    patience_counter = 0
    patience = 15
    
    print("Starting HSV Model Training...")
    print(f"Training on {len(train_loader.dataset)} samples")
    print(f"Validating on {len(val_loader.dataset)} samples")
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')
        
        for batch_idx, (data, target) in enumerate(train_pbar):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            train_correct += pred.eq(target.view_as(pred)).sum().item()
            train_total += target.size(0)
            
            # Update progress bar
            current_acc = 100. * train_correct / train_total
            train_pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_acc:.2f}%'
            })
        
        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = 100. * train_correct / train_total
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')
            for data, target in val_pbar:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                val_loss += loss.item()
                
                # Get predictions
                pred = output.argmax(dim=1, keepdim=True)
                val_correct += pred.eq(target.view_as(pred)).sum().item()
                val_total += target.size(0)
                
                all_preds.extend(pred.cpu().numpy().flatten())
                all_targets.extend(target.cpu().numpy())
                
                current_acc = 100. * val_correct / val_total
                val_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })
        
        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100. * val_correct / val_total
        val_kappa = quadratic_weighted_kappa(all_targets, all_preds)
        
        # Update learning rate
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # Store history
        history['train_losses'].append(avg_train_loss)
        history['val_losses'].append(avg_val_loss)
        history['val_kappas'].append(val_kappa)
        history['val_accuracies'].append(val_accuracy)
        history['learning_rates'].append(current_lr)
        
        # Print epoch results
        print(f'\nEpoch {epoch+1}/{num_epochs}:')
        print(f'  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%')
        print(f'  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%')
        print(f'  Val QWK: {val_kappa:.4f}')
        print(f'  Learning Rate: {current_lr:.8f}')
        
        # Save best model and early stopping
        if val_kappa > best_kappa:
            best_kappa = val_kappa
            best_model_state = model.state_dict().copy()
            torch.save(best_model_state, 'best_hsv_model.pth')
            patience_counter = 0
            print(f'  âœ… New best HSV model saved! QWK: {best_kappa:.4f}')
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f'\nEarly stopping triggered after {patience} epochs without improvement.')
            break
            
        print('-' * 70)
    
    # Load best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    history['best_kappa'] = best_kappa
    history['total_epochs'] = epoch + 1
    
    return model, history

def evaluate_hsv_model(model, test_loader, save_predictions=True):
    """
    Evaluate HSV model on test set and generate Kaggle predictions
    """
    model.eval()
    predictions = []
    image_names = []
    
    print("Generating HSV model predictions...")
    with torch.no_grad():
        for data, names in tqdm(test_loader):
            data = data.to(device)
            outputs = model(data)
            
            # Use softmax probabilities for more confident predictions
            probs = torch.softmax(outputs, dim=1)
            preds = probs.argmax(dim=1)
            
            predictions.extend(preds.cpu().numpy())
            image_names.extend(names)
    
    if save_predictions:
        # Create submission file
        submission_df = pd.DataFrame({
            'id_code': image_names,
            'diagnosis': predictions
        })
        submission_df.to_csv('hsv_predictions.csv', index=False)
        print("HSV predictions saved to 'hsv_predictions.csv'")
    
    return predictions, image_names

def validate_with_confusion_matrix(model, val_loader):
    """
    Generate detailed validation metrics including confusion matrix for HSV model
    """
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in tqdm(val_loader, desc="Generating HSV validation metrics"):
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # Calculate metrics
    qwk = quadratic_weighted_kappa(all_targets, all_preds)
    cm = confusion_matrix(all_targets, all_preds)
    
    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',
                xticklabels=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'],
                yticklabels=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'])
    plt.title(f'HSV Model Confusion Matrix (QWK: {qwk:.4f})')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('hsv_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Classification report
    class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative']
    report = classification_report(all_targets, all_preds, target_names=class_names)
    print("\nHSV Model Classification Report:")
    print(report)
    
    return qwk, cm, report

def plot_hsv_training_history(history):
    """
    Plot comprehensive training history for HSV model
    """
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # Plot losses
    ax1.plot(history['train_losses'], label='Training Loss', color='blue')
    ax1.plot(history['val_losses'], label='Validation Loss', color='orange')
    ax1.set_title('HSV Model: Training and Validation Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # Plot QWK
    ax2.plot(history['val_kappas'], label='Validation QWK', color='green', linewidth=2)
    ax2.axhline(y=0.889, color='red', linestyle='--', label='Target QWK (0.889)', linewidth=2)
    ax2.set_title('HSV Model: Validation Quadratic Weighted Kappa')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('QWK Score')
    ax2.legend()
    ax2.grid(True)
    
    # Plot accuracies
    ax3.plot(history['val_accuracies'], label='Validation Accuracy', color='purple')
    ax3.set_title('HSV Model: Validation Accuracy')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Accuracy (%)')
    ax3.legend()
    ax3.grid(True)
    
    # Plot learning rate
    ax4.plot(history['learning_rates'], label='Learning Rate', color='brown')
    ax4.set_title('HSV Model: Learning Rate Schedule')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Learning Rate')
    ax4.set_yscale('log')
    ax4.legend()
    ax4.grid(True)
    
    plt.tight_layout()
    plt.savefig('hsv_training_history.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Print final results
    print(f"\n{'='*70}")
    print("HSV MODEL TRAINING RESULTS")
    print(f"{'='*70}")
    print(f"Best QWK Score: {history['best_kappa']:.4f}")
    print(f"Total Epochs: {history['total_epochs']}")
    print(f"Final Validation Accuracy: {history['val_accuracies'][-1]:.2f}%")
    
    if history['best_kappa'] > 0.889:
        print("Target QWK score (>0.889) achieved!")
    else:
        print("Target QWK score (>0.889) not yet achieved.")
        print(f"   Gap to target: {0.889 - history['best_kappa']:.4f}")

def analyze_hsv_channels(model, val_loader, num_samples=50):
    """
    Analyze the contribution of individual HSV channels
    """
    model.eval()
    
    print("\nAnalyzing HSV channel contributions...")
    
    # Get a batch of data for analysis
    data_iter = iter(val_loader)
    data, targets = next(data_iter)
    data, targets = data[:num_samples].to(device), targets[:num_samples]
    
    with torch.no_grad():
        # Full HSV prediction
        full_output = model(data)
        full_preds = torch.softmax(full_output, dim=1)
        
        # Analyze each channel individually
        channels = ['Hue', 'Saturation', 'Value']
        channel_results = {}
        
        for i, channel_name in enumerate(channels):
            # Create single-channel input (set other channels to mean values)
            single_channel_data = data.clone()
            
            # Zero out other channels (set to normalized mean)
            for j in range(3):
                if j != i:
                    single_channel_data[:, j, :, :] = 0  # Set to normalized zero
            
            # Get predictions for single channel
            single_output = model(single_channel_data)
            single_preds = torch.softmax(single_output, dim=1)
            
            # Calculate difference in confidence
            confidence_diff = torch.mean(torch.abs(full_preds - single_preds)).item()
            channel_results[channel_name] = confidence_diff
    
    # Plot channel analysis
    plt.figure(figsize=(10, 6))
    channels_list = list(channel_results.keys())
    values = list(channel_results.values())
    colors = ['red', 'green', 'blue']
    
    bars = plt.bar(channels_list, values, color=colors, alpha=0.7)
    plt.title('HSV Channel Contribution Analysis\n(Higher values indicate more important channels)')
    plt.ylabel('Average Prediction Difference')
    plt.xlabel('HSV Channels')
    
    # Add value labels on bars
    for bar, value in zip(bars, values):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('hsv_channel_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("\nHSV Channel Importance:")
    for channel, importance in sorted(channel_results.items(), key=lambda x: x[1], reverse=True):
        print(f"  {channel}: {importance:.4f}")
    
    return channel_results

def compare_with_rgb_baseline(hsv_results, rgb_baseline_path='rgb_experiment_results.json'):
    """
    Compare HSV results with RGB baseline
    """
    try:
        with open(rgb_baseline_path, 'r') as f:
            rgb_results = json.load(f)
        
        print(f"\n{'='*70}")
        print("HSV vs RGB COMPARISON")
        print(f"{'='*70}")
        print(f"RGB QWK:  {rgb_results['best_qwk']:.4f}")
        print(f"HSV QWK:  {hsv_results['best_qwk']:.4f}")
        print(f"Improvement: {hsv_results['best_qwk'] - rgb_results['best_qwk']:.4f}")
        
        if hsv_results['best_qwk'] > rgb_results['best_qwk']:
            print("HSV outperforms RGB!")
        else:
            print("RGB still better than HSV")
            
    except FileNotFoundError:
        print("RGB baseline results not found. Run RGB experiment first for comparison.")

def main():
    """
    Main training pipeline for HSV DR classification (Experiment 2.3)
    """
    # Configuration
    config = {
        'train_csv': '/kaggle/input/aptos2019-blindness-detection/train.csv',
        'test_csv': '/kaggle/input/aptos2019-blindness-detection/test.csv',
        'train_img_dir': '/kaggle/input/aptos2019-blindness-detection/train_images',
        'test_img_dir': '/kaggle/input/aptos2019-blindness-detection/test_images',
        'batch_size': 16,
        'num_epochs': 50,
        'learning_rate': 1e-4,
        'num_classes': 5,
        'dropout_rate': 0.5
    }
    
    print("="*70)
    print("DIABETIC RETINOPATHY HSV MODEL - EXPERIMENT 2.3")
    print("="*70)
    print("Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print("="*70)
    
    # Get HSV-specific transforms
    train_transforms, val_transforms = get_hsv_transforms()
    
    # Load training data with HSV conversion
    print("\nLoading HSV training data...")
    full_train_dataset = DiabeticRetinopathyHSVDataset(
        csv_file=config['train_csv'],
        img_dir=config['train_img_dir'],
        transform=train_transforms
    )
    
    # Split into train and validation (80/20)
    train_size = int(0.8 * len(full_train_dataset))
    val_size = len(full_train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_train_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)  # For reproducibility
    )
    
    # Create separate dataset instance for validation with different transforms
    val_dataset_transform = DiabeticRetinopathyHSVDataset(
        csv_file=config['train_csv'],
        img_dir=config['train_img_dir'],
        transform=val_transforms
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        num_workers=4,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=4,
        pin_memory=True
    )
    
    # Load test data
    test_dataset = DiabeticRetinopathyHSVDataset(
        csv_file=config['test_csv'],
        img_dir=config['test_img_dir'],
        transform=val_transforms,
        is_test=True
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=4,
        pin_memory=True
    )
    
    # Initialize HSV model
    print("\nInitializing HSV model...")
    model = DRHSVClassifier(
        num_classes=config['num_classes'],
        dropout_rate=config['dropout_rate']
    )
    model.to(device)
    
    # Print model information
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    
    # Train HSV model
    print("\nStarting HSV model training...")
    model, history = train_hsv_model(
        model, 
        train_loader, 
        val_loader, 
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate']
    )
    
    # Plot training history
    plot_hsv_training_history(history)
    
    # Detailed validation analysis
    print("\nPerforming detailed validation analysis...")
    final_qwk, cm, report = validate_with_confusion_matrix(model, val_loader)
    
    # Analyze HSV channel contributions
    channel_analysis = analyze_hsv_channels(model, val_loader)
    
    # Generate test predictions
    print("\nGenerating test predictions...")
    predictions, image_names = evaluate_hsv_model(model, test_loader)
    
    # Compile final results
    results = {
        'experiment': 'HSV Model (2.3)',
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'best_qwk': float(history['best_kappa']),
        'final_validation_qwk': float(final_qwk),
        'total_epochs': int(history['total_epochs']),
        'model_architecture': 'ResNet50 + Custom Classifier (Adapted for HSV)',
        'input_type': 'HSV Color Space (3-channel)',
        'preprocessing': 'RGB to HSV conversion with custom normalization',
        'image_size': '512x512',
        'batch_size': config['batch_size'],
        'learning_rate': config['learning_rate'],
        'optimizer': 'AdamW with CosineAnnealingWarmRestarts',
        'augmentations': 'Flip, Rotation, Affine, ResizedCrop (HSV-adapted)',
        'target_achieved': bool(history['best_kappa'] > 0.889),
        'channel_analysis': channel_analysis
    }
    
    # Save results
    with open('hsv_experiment_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # Compare with RGB baseline
    compare_with_rgb_baseline(results)
    
    # Print final summary
    print("\n" + "="*70)
    print("HSV MODEL EXPERIMENT 2.3 - FINAL RESULTS SUMMARY")
    print("="*70)
    for key, value in results.items():
        if key != 'channel_analysis':  # Skip detailed channel analysis in summary
            print(f"{key}: {value}")
    print("="*70)
    
    print(f"\nFiles saved:")
    print(f"  - Model weights: 'best_hsv_model.pth'")
    print(f"  - Predictions: 'hsv_predictions.csv'")
    print(f"  - Training plots: 'hsv_training_history.png'")
    print(f"  - Confusion matrix: 'hsv_confusion_matrix.png'")
    print(f"  - Channel analysis: 'hsv_channel_analysis.png'")
    print(f"  - Results: 'hsv_experiment_results.json'")
    
    return model, history, results

if __name__ == "__main__":
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # Run HSV experiment
    model, history, results = main()
