import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import json
from datetime import datetime
import random
from scipy.ndimage import gaussian_filter, map_coordinates
warnings.filterwarnings('ignore')

# Import Swin Transformer
try:
    from transformers import SwinForImageClassification, SwinConfig
    from transformers import AutoImageProcessor
except ImportError:
    print("Installing transformers library...")
    import subprocess
    subprocess.run(["pip", "install", "transformers"])
    from transformers import SwinForImageClassification, SwinConfig
    from transformers import AutoImageProcessor

# Import timm for better Swin implementation
try:
    import timm
except ImportError:
    print("Installing timm library...")
    import subprocess
    subprocess.run(["pip", "install", "timm"])
    import timm

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

def crop_black_background(image, threshold=10):
    """
    Crop black background from retinal images
    """
    # Convert to grayscale for processing
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    else:
        gray = image
    
    # Create mask for non-black pixels
    mask = gray > threshold
    
    # Find bounding box of non-black region
    coords = np.argwhere(mask)
    if len(coords) == 0:
        return image  # Return original if no non-black pixels found
    
    y0, x0 = coords.min(axis=0)
    y1, x1 = coords.max(axis=0) + 1
    
    # Crop the image
    cropped = image[y0:y1, x0:x1]
    
    return cropped

class RetinaSpecificAugmentations:
    """
    Augmentations specifically designed for retinal images
    """
    def __init__(self, is_training=True):
        self.is_training = is_training
        
    def __call__(self, image):
        if not self.is_training:
            return image
            
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        # Add vasculature-like augmentations
        if random.random() < 0.4:
            # Simulate different camera exposures
            exposure_factor = random.uniform(0.8, 1.2)
            image = cv2.convertScaleAbs(image, alpha=exposure_factor, beta=0)
            
        if random.random() < 0.3:
            # Add slight blur to simulate focus issues
            kernel_size = random.choice([3, 5])
            image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)
            
        if random.random() < 0.3:
            # Add slight noise
            noise = np.random.normal(0, 5, image.shape).astype(np.uint8)
            image = cv2.add(image, noise)
            
        if random.random() < 0.4:
            # Color channel adjustments (common in retinal photography)
            for i in range(3):
                adjustment = random.uniform(0.9, 1.1)
                image[:, :, i] = np.clip(image[:, :, i] * adjustment, 0, 255).astype(np.uint8)
                
        return image

class CustomAugmentations:
    """
    Custom augmentation pipeline
    """
    def __init__(self, is_training=True):
        self.is_training = is_training
        self.retina_augs = RetinaSpecificAugmentations(is_training)
    
    def __call__(self, image):
        if not self.is_training:
            return image
        
        # Convert PIL to numpy if needed
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        # Apply augmentations with certain probabilities
        if random.random() < 0.5:
            # Horizontal flip
            image = cv2.flip(image, 1)
        
        if random.random() < 0.5:
            # Vertical flip
            image = cv2.flip(image, 0)
        
        if random.random() < 0.7:
            # Random rotation (0-360 degrees)
            angle = random.uniform(0, 360)
            h, w = image.shape[:2]
            center = (w // 2, h // 2)
            matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            image = cv2.warpAffine(image, matrix, (w, h))
        
        if random.random() < 0.5:
            # Zoom (0.8 to 1.2)
            zoom_factor = random.uniform(0.8, 1.2)
            h, w = image.shape[:2]
            new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)
            
            # Resize image
            resized = cv2.resize(image, (new_w, new_h))
            
            # Crop or pad to original size
            if zoom_factor > 1.0:
                # Crop center
                start_x = (new_w - w) // 2
                start_y = (new_h - h) // 2
                image = resized[start_y:start_y+h, start_x:start_x+w]
            else:
                # Pad
                pad_x = (w - new_w) // 2
                pad_y = (h - new_h) // 2
                image = cv2.copyMakeBorder(resized, pad_y, pad_y, pad_x, pad_x, cv2.BORDER_CONSTANT)
        
        # Apply retina-specific augmentations
        image = self.retina_augs(image)
        
        return image

class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss
        
        if self.alpha is not None:
            alpha_t = self.alpha[targets]
            focal_loss = alpha_t * focal_loss
            
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class DiabeticRetinopathyRGBYCbCrDataset(Dataset):
    """
    Enhanced Dataset for Diabetic Retinopathy with RGB + YCbCr fusion
    """
    def __init__(self, csv_file, img_dir, transform=None, is_test=False, image_size=224):
        """
        Args:
            csv_file (string): Path to csv with image names and labels
            img_dir (string): Directory with all images
            transform (callable, optional): Transform to be applied on images
            is_test (bool): Whether this is test dataset (no labels)
            image_size (int): Target image size (224x224 for Swin)
        """
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        self.is_test = is_test
        self.image_size = image_size
        
        # Print dataset statistics
        if not is_test:
            print(f"Dataset loaded: {len(self.data)} images")
            label_counts = self.data.iloc[:, 1].value_counts().sort_index()
            print("Label distribution:")
            for label, count in label_counts.items():
                print(f"  Class {label}: {count} images ({count/len(self.data)*100:.1f}%)")
        
    def __len__(self):
        return len(self.data)
    
    def preprocess_image(self, image_path):
        """
        Enhanced preprocessing with black background cropping
        """
        # Load image
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Could not load image: {image_path}")
        
        # Convert BGR to RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Crop black background
        image = crop_black_background(image, threshold=10)
        
        # Resize to target size
        image = cv2.resize(image, (self.image_size, self.image_size))
        
        return image
    
    def rgb_to_ycbcr(self, rgb_image):
        """
        Convert RGB image to YCbCr using OpenCV
        """
        ycbcr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2YCrCb)
        
        # Note: OpenCV returns YCrCb, but we want YCbCr, so we need to swap channels
        y_channel = ycbcr_image[:, :, 0]
        cr_channel = ycbcr_image[:, :, 1]  # This is Cr
        cb_channel = ycbcr_image[:, :, 2]  # This is Cb
        
        # Rearrange to YCbCr format
        ycbcr_correct = np.stack([y_channel, cb_channel, cr_channel], axis=2)
        return ycbcr_correct
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name
        img_name = self.data.iloc[idx, 0]
        
        # Try different extensions
        img_extensions = ['.jpeg', '.jpg', '.png']
        img_path = None
        
        for ext in img_extensions:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension")
        
        # Load and preprocess image
        rgb_image = self.preprocess_image(img_path)
        
        # Convert to YCbCr
        ycbcr_image = self.rgb_to_ycbcr(rgb_image)
        
        # Apply transforms
        if self.transform:
            # Apply same augmentation to both RGB and YCbCr
            random_state = random.getstate()
            
            rgb_pil = Image.fromarray(rgb_image)
            rgb_tensor = self.transform(rgb_pil)
            
            # Restore random state and apply same augmentation to YCbCr
            random.setstate(random_state)
            ycbcr_pil = Image.fromarray(ycbcr_image)
            ycbcr_tensor = self.transform(ycbcr_pil)
        else:
            # Convert to tensors without transform
            rgb_tensor = torch.from_numpy(rgb_image.transpose(2, 0, 1)).float() / 255.0
            ycbcr_tensor = torch.from_numpy(ycbcr_image.transpose(2, 0, 1)).float() / 255.0
            
            # Normalize both with ImageNet stats
            rgb_tensor = transforms.Normalize(
                mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225]
            )(rgb_tensor)
            
            ycbcr_tensor = transforms.Normalize(
                mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225]
            )(ycbcr_tensor)
        
        if not self.is_test:
            label = int(self.data.iloc[idx, 1])
            return (rgb_tensor, ycbcr_tensor), label
        else:
            return (rgb_tensor, ycbcr_tensor), img_name

class SwinFeatureExtractor(nn.Module):
    """
    Swin Transformer feature extractor using timm for better performance
    """
    def __init__(self, model_name="swin_base_patch4_window7_224", pretrained=True, in_chans=3):
        super(SwinFeatureExtractor, self).__init__()
        
        # Use timm for better Swin implementation
        self.swin = timm.create_model(
            model_name, 
            pretrained=pretrained, 
            num_classes=0,  # We'll add our own classifier
            in_chans=in_chans
        )
        
        # Get the number of features
        self.num_features = self.swin.num_features
        
    def forward(self, x):
        return self.swin(x)

class EnhancedSwinDualStreamFusion(nn.Module):
    """
    Enhanced fusion with Swin Transformer and attention mechanism
    """
    def __init__(self, num_classes=5, dropout_rate=0.5):
        super(EnhancedSwinDualStreamFusion, self).__init__()
        
        # RGB stream with Swin Transformer
        self.rgb_backbone = SwinFeatureExtractor(in_chans=3)
        
        # YCbCr stream with Swin Transformer
        self.ycbcr_backbone = SwinFeatureExtractor(in_chans=3)
        
        # Get feature dimensions
        rgb_features = self.rgb_backbone.num_features
        ycbcr_features = self.ycbcr_backbone.num_features
        
        print(f"RGB features dimension: {rgb_features}")
        print(f"YCbCr features dimension: {ycbcr_features}")
        
        # Attention mechanism for feature fusion
        self.attention = nn.Sequential(
            nn.Linear(rgb_features + ycbcr_features, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, 2),
            nn.Softmax(dim=1)
        )
        
        # Final classifier
        self.classifier = nn.Sequential(
            nn.LayerNorm(rgb_features + ycbcr_features),
            nn.Dropout(dropout_rate),
            nn.Linear(rgb_features + ycbcr_features, 1024),
            nn.GELU(),
            nn.Dropout(dropout_rate/2),
            nn.Linear(1024, 512),
            nn.GELU(),
            nn.Dropout(dropout_rate/2),
            nn.Linear(512, num_classes)
        )
        
        # Initialize weights
        self._init_weights()
        
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)
        
    def forward(self, x):
        rgb, ycbcr = x
        
        rgb_features = self.rgb_backbone(rgb)
        ycbcr_features = self.ycbcr_backbone(ycbcr)
        
        # Concatenate features
        combined = torch.cat([rgb_features, ycbcr_features], dim=1)
        
        # Calculate attention weights
        attention_weights = self.attention(combined)
        
        # Apply attention
        weighted_rgb = attention_weights[:, 0:1] * rgb_features
        weighted_ycbcr = attention_weights[:, 1:2] * ycbcr_features
        
        # Final fusion
        fused_features = torch.cat([weighted_rgb, weighted_ycbcr], dim=1)
        
        return self.classifier(fused_features)

class EnhancedTransforms:
    """
    Enhanced transforms optimized for Swin Transformer
    """
    def __init__(self, size=224, is_training=True):
        self.size = size
        self.is_training = is_training
        
        if is_training:
            # Training transforms
            self.transform = transforms.Compose([
                transforms.Resize((size, size)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomVerticalFlip(p=0.3),
                transforms.RandomRotation(degrees=15),
                transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),
                transforms.RandomResizedCrop(size, scale=(0.8, 1.0)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            # Validation transforms
            self.transform = transforms.Compose([
                transforms.Resize((size, size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
    
    def __call__(self, image):
        return self.transform(image)

def get_enhanced_transforms():
    """
    Get enhanced transforms for training and validation
    """
    train_transforms = EnhancedTransforms(size=224, is_training=True)
    val_transforms = EnhancedTransforms(size=224, is_training=False)
    
    return train_transforms, val_transforms

def quadratic_weighted_kappa(y_true, y_pred):
    """
    Calculate Quadratic Weighted Kappa (QWK) score
    """
    return cohen_kappa_score(y_true, y_pred, weights='quadratic')

def get_optimizer_and_scheduler(model, train_loader, num_epochs, learning_rate):
    # Separate parameters for differential learning rates
    rgb_backbone_params = []
    ycbcr_backbone_params = []
    classifier_params = []
    
    for name, param in model.named_parameters():
        if 'rgb_backbone' in name:
            rgb_backbone_params.append(param)
        elif 'ycbcr_backbone' in name:
            ycbcr_backbone_params.append(param)
        else:
            classifier_params.append(param)
    
    optimizer = optim.AdamW([
        {'params': rgb_backbone_params, 'lr': learning_rate * 0.1},
        {'params': ycbcr_backbone_params, 'lr': learning_rate * 0.1},
        {'params': classifier_params, 'lr': learning_rate}
    ], weight_decay=1e-4)
    
    # Cosine annealing with warm restarts
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, 
        T_0=len(train_loader) * 5,  # 5 epochs per restart
        T_mult=2,
        eta_min=learning_rate * 0.01
    )
    
    return optimizer, scheduler

def train_dual_stream_model(model, train_loader, val_loader, num_epochs=60, learning_rate=1e-4):
    """
    Train the dual-stream RGB+YCbCr fusion model
    """
    # Use Focal Loss for class imbalance
    class_weights = torch.tensor([1.0, 2.5, 1.2, 4.0, 2.0]).to(device)
    criterion = FocalLoss(alpha=class_weights, gamma=2.0)
    
    # Get enhanced optimizer and scheduler
    optimizer, scheduler = get_optimizer_and_scheduler(model, train_loader, num_epochs, learning_rate)
    
    # Training history
    history = {
        'train_losses': [],
        'val_losses': [],
        'val_kappas': [],
        'val_accuracies': [],
        'learning_rates': []
    }
    
    best_kappa = 0.0
    best_model_state = None
    patience_counter = 0
    patience = 20
    
    print("Starting Enhanced Dual-Stream RGB+YCbCr Fusion Model Training...")
    print(f"Training on {len(train_loader.dataset)} samples")
    print(f"Validating on {len(val_loader.dataset)} samples")
    print(f"Image size: 224x224")
    print(f"Architecture: Enhanced Dual-stream Swin Transformer (RGB + YCbCr)")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')
        
        for batch_idx, (data, target) in enumerate(train_pbar):
            # data is a tuple of (rgb_tensor, ycbcr_tensor)
            rgb_data, ycbcr_data = data
            rgb_data = rgb_data.to(device)
            ycbcr_data = ycbcr_data.to(device)
            target = target.to(device)
            
            # Skip batches with size 1 to avoid BatchNorm issues
            if rgb_data.size(0) == 1:
                continue
            
            optimizer.zero_grad()
            output = model((rgb_data, ycbcr_data))
            loss = criterion(output, target)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            train_correct += pred.eq(target.view_as(pred)).sum().item()
            train_total += target.size(0)
            
            # Update progress bar
            current_acc = 100. * train_correct / train_total
            train_pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_acc:.2f}%',
                'LR': f'{scheduler.get_last_lr()[0]:.2e}'
            })
        
        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = 100. * train_correct / train_total
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')
            for data, target in val_pbar:
                rgb_data, ycbcr_data = data
                rgb_data = rgb_data.to(device)
                ycbcr_data = ycbcr_data.to(device)
                target = target.to(device)
                
                if rgb_data.size(0) == 1:
                    continue
                    
                output = model((rgb_data, ycbcr_data))
                loss = criterion(output, target)
                val_loss += loss.item()
                
                pred = output.argmax(dim=1, keepdim=True)
                val_correct += pred.eq(target.view_as(pred)).sum().item()
                val_total += target.size(0)
                
                all_preds.extend(pred.cpu().numpy().flatten())
                all_targets.extend(target.cpu().numpy())
                
                current_acc = 100. * val_correct / val_total
                val_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })
        
        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100. * val_correct / val_total
        val_kappa = quadratic_weighted_kappa(all_targets, all_preds)
        
        current_lr = scheduler.get_last_lr()[0]
        
        # Store history
        history['train_losses'].append(avg_train_loss)
        history['val_losses'].append(avg_val_loss)
        history['val_kappas'].append(val_kappa)
        history['val_accuracies'].append(val_accuracy)
        history['learning_rates'].append(current_lr)
        
        # Print epoch results
        print(f'\nEpoch {epoch+1}/{num_epochs}:')
        print(f'  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%')
        print(f'  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%')
        print(f'  Val QWK: {val_kappa:.4f}')
        print(f'  Learning Rate: {current_lr:.8f}')
        
        # Save best model
        if val_kappa > best_kappa:
            best_kappa = val_kappa
            best_model_state = model.state_dict().copy()
            torch.save(best_model_state, 'best_swin_dual_stream_model.pth')
            patience_counter = 0
            print(f'  âœ… New best model saved! QWK: {best_kappa:.4f}')
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f'\nEarly stopping triggered after {patience} epochs without improvement.')
            break
            
        print('-' * 70)
    
    # Load best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    history['best_kappa'] = best_kappa
    history['total_epochs'] = epoch + 1
    
    return model, history

def main():
    """
    Main training pipeline for dual-stream RGB+YCbCr fusion model
    """
    # Configuration
    config = {
        'train_csv': '/kaggle/input/aptos2019-blindness-detection/train.csv',
        'test_csv': '/kaggle/input/aptos2019-blindness-detection/test.csv',
        'train_img_dir': '/kaggle/input/aptos2019-blindness-detection/train_images',
        'test_img_dir': '/kaggle/input/aptos2019-blindness-detection/test_images',
        'batch_size': 8,  # Reduced for Swin Transformer
        'num_epochs': 60,  # Increased epochs
        'learning_rate': 1e-4,  # Adjusted learning rate
        'num_classes': 5,
        'dropout_rate': 0.5,
        'image_size': 224  # Changed to 224 for Swin
    }
    
    print("="*70)
    print("ENHANCED DUAL-STREAM DIABETIC RETINOPATHY RGB+YCbCr FUSION MODEL")
    print("WITH SWIN TRANSFORMER (FIXED)")
    print("="*70)
    print("Key Fixes Applied:")
    print("  - Using timm library for better Swin implementation")
    print("  - Fixed YCbCr channel ordering (Y, Cb, Cr)")
    print("  - Changed image size to 224x224 for Swin")
    print("  - Improved data preprocessing pipeline")
    print("  - Better weight initialization")
    print("  - Increased training epochs")
    print("="*70)
    
    # Get enhanced transforms
    train_transforms, val_transforms = get_enhanced_transforms()
    
    # Load training data
    print("\nLoading dual-stream RGB+YCbCr training data...")
    full_train_dataset = DiabeticRetinopathyRGBYCbCrDataset(
        csv_file=config['train_csv'],
        img_dir=config['train_img_dir'],
        transform=train_transforms,
        image_size=config['image_size']
    )
    
    # Split into train and validation
    train_size = int(0.8 * len(full_train_dataset))
    val_size = len(full_train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_train_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        num_workers=4,
        pin_memory=True,
        drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=4,
        pin_memory=True,
        drop_last=False
    )
    
    # Initialize enhanced dual-stream model with Swin Transformer
    print("\nInitializing enhanced dual-stream RGB+YCbCr fusion model with Swin Transformer...")
    model = EnhancedSwinDualStreamFusion(
        num_classes=config['num_classes'],
        dropout_rate=config['dropout_rate']
    )
    model.to(device)
    
    # Print model information
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    
    # Train enhanced dual-stream model
    print("\nStarting enhanced dual-stream RGB+YCbCr fusion model training...")
    model, history = train_dual_stream_model(
        model, 
        train_loader, 
        val_loader, 
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate']
    )
    
    print(f"\nTraining completed!")
    print(f"Best QWK: {history['best_kappa']:.4f}")
    print(f"Total epochs: {history['total_epochs']}")
    
    # Evaluate on validation set
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in val_loader:
            rgb_data, ycbcr_data = data
            rgb_data = rgb_data.to(device)
            ycbcr_data = ycbcr_data.to(device)
            target = target.to(device)
            
            output = model((rgb_data, ycbcr_data))
            pred = output.argmax(dim=1)
            
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    final_kappa = quadratic_weighted_kappa(all_targets, all_preds)
    final_accuracy = 100.0 * (np.array(all_preds) == np.array(all_targets)).mean()
    
    print(f"\nFinal Validation Results:")
    print(f"  Accuracy: {final_accuracy:.2f}%")
    print(f"  QWK: {final_kappa:.4f}")
    
    # Confusion matrix
    cm = confusion_matrix(all_targets, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix (QWK: {final_kappa:.4f})')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return model, history

if __name__ == "__main__":
    # Run enhanced dual-stream RGB+YCbCr fusion experiment with Swin Transformer
    model, history = main()
