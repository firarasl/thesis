import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import json
from datetime import datetime
warnings.filterwarnings('ignore')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class DiabeticRetinopathyHVChannelDataset(Dataset):
    """
    Custom Dataset for Diabetic Retinopathy with HV (Hue-Value) channels only
    """
    def __init__(self, csv_file, img_dir, transform=None, is_test=False):
        """
        Args:
            csv_file (string): Path to csv with image names and labels
            img_dir (string): Directory with all images
            transform (callable, optional): Transform to be applied on images
            is_test (bool): Whether this is test dataset (no labels)
        """
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        self.is_test = is_test
        
        # Print dataset statistics
        if not is_test:
            print(f"HV Channel Dataset loaded: {len(self.data)} images")
            print(f"Output channels: 2 (Hue + Value)")
            label_counts = self.data.iloc[:, 1].value_counts().sort_index()
            print("Label distribution:")
            for label, count in label_counts.items():
                print(f"  Class {label}: {count} images ({count/len(self.data)*100:.1f}%)")
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name
        img_name = self.data.iloc[idx, 0]
        
        # Try different extensions
        img_extensions = ['.jpeg', '.jpg', '.png']
        img_path = None
        
        for ext in img_extensions:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension")
        
        # Load image
        image = cv2.imread(img_path)
        if image is None:
            raise ValueError(f"Could not load image: {img_path}")
        
        # Convert BGR to RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Convert RGB to HSV
        image_hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)
        
        # Extract Hue and Value channels (H and V)
        image_hv = image_hsv[:, :, [0, 2]]  # H, V channels
        
        # Convert to PIL Image for transforms
        image_pil = Image.fromarray(image_hv.astype('uint8'))
        
        if self.transform:
            image_pil = self.transform(image_pil)
            
        if not self.is_test:
            label = int(self.data.iloc[idx, 1])
            return image_pil, label
        else:
            return image_pil, img_name

class DREfficientNetB2HVClassifier(nn.Module):
    """
    Diabetic Retinopathy Classifier using EfficientNet-B2 for HV (Hue-Value) channels
    """
    def __init__(self, num_classes=5, pretrained=True, dropout_rate=0.4):
        super(DREfficientNetB2HVClassifier, self).__init__()
        
        # Use EfficientNet-B2 as backbone
        self.backbone = models.efficientnet_b2(pretrained=pretrained)
        
        # Store original first conv layer for weight adaptation
        original_conv = self.backbone.features[0][0]
        
        # Modify first conv layer for 2 input channels (HV)
        self.backbone.features[0][0] = nn.Conv2d(
            2, original_conv.out_channels, 
            kernel_size=original_conv.kernel_size,
            stride=original_conv.stride,
            padding=original_conv.padding,
            bias=original_conv.bias is not None
        )
        
        # Initialize new conv1 weights
        if pretrained:
            with torch.no_grad():
                # Average pretrained RGB weights for HV channels initialization
                rgb_weights = original_conv.weight
                avg_weights = rgb_weights.mean(dim=1, keepdim=True)  # Average across RGB channels
                # Initialize with averaged weights repeated for both HV channels
                self.backbone.features[0][0].weight = nn.Parameter(
                    torch.cat([avg_weights, avg_weights], dim=1)
                )
                
                # Copy bias if exists
                if original_conv.bias is not None:
                    self.backbone.features[0][0].bias = nn.Parameter(original_conv.bias.clone())
            print("Adapted pretrained EfficientNet-B2 weights for HV (Hue-Value) channels")
        else:
            # Initialize with He initialization
            nn.init.kaiming_normal_(self.backbone.features[0][0].weight, mode='fan_out', nonlinearity='relu')
            if self.backbone.features[0][0].bias is not None:
                nn.init.constant_(self.backbone.features[0][0].bias, 0)
        
        # Get the number of features in the classifier
        num_features = self.backbone.classifier[1].in_features
        
        # Replace final layer with custom classifier
        self.backbone.classifier = nn.Sequential(
            nn.Dropout(p=dropout_rate),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate/2),
            nn.Linear(512, num_classes)
        )
        
        # Initialize weights for new layers
        for m in self.backbone.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        return self.backbone(x)

def get_hv_transforms():
    """
    Define transforms for HV (Hue-Value) channels optimized for EfficientNet
    """
    # Two-channel normalization for HV channels
    # Hue: 0-179 in OpenCV HSV, Value: 0-255
    # Normalized values (approximate)
    mean = [0.5, 0.5]  # H, V channel means (normalized)
    std = [0.3, 0.3]   # H, V channel stds (normalized)
    
    train_transforms = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),
        transforms.RandomResizedCrop(512, scale=(0.8, 1.0)),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])
    
    val_transforms = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])
    
    return train_transforms, val_transforms

def quadratic_weighted_kappa(y_true, y_pred):
    """Calculate Quadratic Weighted Kappa (QWK) score"""
    return cohen_kappa_score(y_true, y_pred, weights='quadratic')

def train_hv_effnet_model(model, train_loader, val_loader, num_epochs=50, learning_rate=2e-4):
    """
    Train HV channel model with EfficientNet-B2
    """
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )
    
    history = {
        'train_losses': [],
        'val_losses': [],
        'val_kappas': [],
        'val_accuracies': [],
        'learning_rates': []
    }
    
    best_kappa = 0.0
    best_model_state = None
    patience_counter = 0
    patience = 15
    
    print(f"Starting HV Channel EfficientNet-B2 Model Training...")
    print(f"Training on {len(train_loader.dataset)} samples")
    print(f"Validating on {len(val_loader.dataset)} samples")
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [HV-EffNet] Train')
        
        for batch_idx, (data, target) in enumerate(train_pbar):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            train_correct += pred.eq(target.view_as(pred)).sum().item()
            train_total += target.size(0)
            
            current_acc = 100. * train_correct / train_total
            train_pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_acc:.2f}%'
            })
        
        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = 100. * train_correct / train_total
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [HV-EffNet] Val')
            for data, target in val_pbar:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                val_loss += loss.item()
                
                pred = output.argmax(dim=1, keepdim=True)
                val_correct += pred.eq(target.view_as(pred)).sum().item()
                val_total += target.size(0)
                
                all_preds.extend(pred.cpu().numpy().flatten())
                all_targets.extend(target.cpu().numpy())
                
                current_acc = 100. * val_correct / val_total
                val_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })
        
        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100. * val_correct / val_total
        val_kappa = quadratic_weighted_kappa(all_targets, all_preds)
        
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # Store history
        history['train_losses'].append(avg_train_loss)
        history['val_losses'].append(avg_val_loss)
        history['val_kappas'].append(val_kappa)
        history['val_accuracies'].append(val_accuracy)
        history['learning_rates'].append(current_lr)
        
        # Print epoch results
        print(f'\nEpoch {epoch+1}/{num_epochs} [HV-EffNet]:')
        print(f'  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%')
        print(f'  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%')
        print(f'  Val QWK: {val_kappa:.4f}')
        print(f'  Learning Rate: {current_lr:.8f}')
        
        # Save best model
        if val_kappa > best_kappa:
            best_kappa = val_kappa
            best_model_state = model.state_dict().copy()
            torch.save(best_model_state, 'best_hv_effnet_model.pth')
            patience_counter = 0
            print(f'  ‚úÖ New best HV EfficientNet model saved! QWK: {best_kappa:.4f}')
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f'\nEarly stopping triggered after {patience} epochs without improvement.')
            break
            
        print('-' * 70)
    
    # Load best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    history['best_kappa'] = best_kappa
    history['total_epochs'] = epoch + 1
    
    return model, history

def evaluate_hv_effnet_model(model, test_loader):
    """
    Evaluate HV EfficientNet model on test set
    """
    model.eval()
    predictions = []
    image_names = []
    
    print("Generating HV EfficientNet model predictions...")
    with torch.no_grad():
        for data, names in tqdm(test_loader):
            data = data.to(device)
            outputs = model(data)
            probs = torch.softmax(outputs, dim=1)
            preds = probs.argmax(dim=1)
            
            predictions.extend(preds.cpu().numpy())
            image_names.extend(names)
    
    # Create submission file
    submission_df = pd.DataFrame({
        'id_code': image_names,
        'diagnosis': predictions
    })
    submission_df.to_csv('hv_effnet_predictions.csv', index=False)
    print("HV EfficientNet predictions saved to 'hv_effnet_predictions.csv'")
    
    return predictions, image_names

def plot_hv_effnet_training_curves(history):
    """
    Plot training curves for HV EfficientNet model
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    epochs = range(1, len(history['train_losses']) + 1)
    
    # Loss curves
    axes[0, 0].plot(epochs, history['train_losses'], 'b-', label='Training Loss')
    axes[0, 0].plot(epochs, history['val_losses'], 'r-', label='Validation Loss')
    axes[0, 0].set_title('HV EfficientNet-B2 - Training and Validation Loss')
    axes[0, 0].set_xlabel('Epochs')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # QWK score
    axes[0, 1].plot(epochs, history['val_kappas'], 'g-', label='Validation QWK')
    axes[0, 1].axhline(y=0.889, color='black', linestyle='--', label='Target QWK (0.889)')
    axes[0, 1].set_title('HV EfficientNet-B2 - Validation QWK Score')
    axes[0, 1].set_xlabel('Epochs')
    axes[0, 1].set_ylabel('QWK Score')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # Accuracy
    axes[1, 0].plot(epochs, history['val_accuracies'], 'purple', label='Validation Accuracy')
    axes[1, 0].set_title('HV EfficientNet-B2 - Validation Accuracy')
    axes[1, 0].set_xlabel('Epochs')
    axes[1, 0].set_ylabel('Accuracy (%)')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # Learning rate
    axes[1, 1].plot(epochs, history['learning_rates'], 'orange', label='Learning Rate')
    axes[1, 1].set_title('HV EfficientNet-B2 - Learning Rate Schedule')
    axes[1, 1].set_xlabel('Epochs')
    axes[1, 1].set_ylabel('Learning Rate')
    axes[1, 1].set_yscale('log')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig('hv_effnet_training_curves.png', dpi=300, bbox_inches='tight')
    plt.show()

def validate_hv_effnet_with_confusion_matrix(model, val_loader):
    """
    Generate detailed validation metrics including confusion matrix for HV EfficientNet
    """
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in tqdm(val_loader, desc="Generating validation metrics"):
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # Calculate metrics
    qwk = quadratic_weighted_kappa(all_targets, all_preds)
    cm = confusion_matrix(all_targets, all_preds)
    
    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'],
                yticklabels=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'])
    plt.title(f'HV EfficientNet-B2 Confusion Matrix (QWK: {qwk:.4f})')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('hv_effnet_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Classification report
    class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative']
    report = classification_report(all_targets, all_preds, target_names=class_names)
    print("\nClassification Report:")
    print(report)
    
    return qwk, cm, report

def main():
    """
    Main function to run HV Channel EfficientNet-B2 Model Training
    """
    # Configuration
    config = {
        'train_csv': '/kaggle/input/aptos2019-blindness-detection/train.csv',
        'test_csv': '/kaggle/input/aptos2019-blindness-detection/test.csv',
        'train_img_dir': '/kaggle/input/aptos2019-blindness-detection/train_images',
        'test_img_dir': '/kaggle/input/aptos2019-blindness-detection/test_images',
        'batch_size': 16,
        'num_epochs': 50,
        'learning_rate': 2e-4,  # Higher learning rate for EfficientNet
        'num_classes': 5,
        'dropout_rate': 0.4     # Lower dropout for EfficientNet
    }
    
    print("="*70)
    print("DIABETIC RETINOPATHY HV (HUE-VALUE) CHANNEL EFFICIENTNET-B2 MODEL")
    print("="*70)
    
    # Get transforms
    train_transforms, val_transforms = get_hv_transforms()
    
    # Load training data
    full_train_dataset = DiabeticRetinopathyHVChannelDataset(
        csv_file=config['train_csv'],
        img_dir=config['train_img_dir'],
        transform=train_transforms
    )
    
    # Split into train and validation
    train_size = int(0.8 * len(full_train_dataset))
    val_size = len(full_train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_train_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, batch_size=config['batch_size'], shuffle=True, 
        num_workers=4, pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=config['batch_size'], shuffle=False, 
        num_workers=4, pin_memory=True
    )
    
    # Load test data
    test_dataset = DiabeticRetinopathyHVChannelDataset(
        csv_file=config['test_csv'],
        img_dir=config['test_img_dir'],
        transform=val_transforms,
        is_test=True
    )
    test_loader = DataLoader(
        test_dataset, batch_size=config['batch_size'], shuffle=False,
        num_workers=4, pin_memory=True
    )
    
    # Initialize model
    model = DREfficientNetB2HVClassifier(
        num_classes=config['num_classes'],
        dropout_rate=config['dropout_rate']
    )
    model.to(device)
    
    # Print model information
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model initialized with {total_params:,} total parameters")
    print(f"Trainable parameters: {trainable_params:,}")
    
    # Train model
    model, history = train_hv_effnet_model(
        model, train_loader, val_loader,
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate']
    )
    
    # Generate predictions
    predictions, image_names = evaluate_hv_effnet_model(model, test_loader)
    
    # Plot training curves
    plot_hv_effnet_training_curves(history)
    
    # Detailed validation analysis
    print("\nPerforming detailed validation analysis...")
    final_qwk, cm, report = validate_hv_effnet_with_confusion_matrix(model, val_loader)
    
    # Save results
    results = {
        'model': 'HV Channel EfficientNet-B2 (Hue-Value)',
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'best_qwk': float(history['best_kappa']),
        'final_validation_qwk': float(final_qwk),
        'total_epochs': int(history['total_epochs']),
        'final_val_acc': float(history['val_accuracies'][-1]),
        'model_architecture': 'EfficientNet-B2 + Custom Classifier',
        'input_type': 'HV Channels (2-channel)',
        'preprocessing': 'HSV color space - Hue and Value channels only',
        'image_size': '512x512',
        'target_achieved': bool(history['best_kappa'] > 0.889),
        'config': config
    }
    
    # Save to JSON file
    with open('hv_effnet_model_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n{'='*70}")
    print("HV EFFICIENTNET-B2 MODEL TRAINING COMPLETED")
    print(f"{'='*70}")
    print(f"Best QWK Score: {history['best_kappa']:.4f}")
    print(f"Final Validation QWK: {final_qwk:.4f}")
    print(f"Final Validation Accuracy: {history['val_accuracies'][-1]:.2f}%")
    print(f"Training Epochs: {history['total_epochs']}")
    print(f"Target Achieved: {'‚úÖ Yes' if history['best_kappa'] > 0.889 else '‚ùå No'}")
    print(f"\nFiles saved:")
    print(f"  - Model weights: 'best_hv_effnet_model.pth'")
    print(f"  - Predictions: 'hv_effnet_predictions.csv'")
    print(f"  - Training plots: 'hv_effnet_training_curves.png'")
    print(f"  - Confusion matrix: 'hv_effnet_confusion_matrix.png'")
    print(f"  - Results: 'hv_effnet_model_results.json'")
    
    return model, history, results

# Run the experiment
if __name__ == "__main__":
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    model, history, results = main()
    print("\nüéâ HV Channel EfficientNet-B2 Model Training Completed Successfully!")
