import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from tqdm import tqdm
import warnings
import json
warnings.filterwarnings('ignore')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Import timm for PVTv2
import timm

class DiabeticRetinopathyRGDataset(Dataset):
    """
    Custom Dataset for Diabetic Retinopathy with RG (Red-Green) images (Test Dataset)
    """
    def __init__(self, csv_file, img_dir, transform=None):
        """
        Args:
            csv_file (string): Path to csv with image names
            img_dir (string): Directory with all images
            transform (callable, optional): Transform to be applied on images
        """
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        
        print(f"Test dataset loaded: {len(self.data)} images")
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name (assuming the CSV has 'id_code' column)
        img_name = str(self.data.iloc[idx, 0])  # First column should be image ID
        
        # Try different extensions
        img_extensions = ['.png', '.jpg', '.jpeg']
        img_path = None
        
        for ext in img_extensions:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension")
        
        try:
            # Load image
            image = cv2.imread(img_path)
            if image is None:
                raise ValueError(f"Could not load image: {img_path}")
            
            # Convert BGR to RGB
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Extract Red and Green channels only
            image_rg = np.zeros((image_rgb.shape[0], image_rgb.shape[1], 2), dtype=np.uint8)
            image_rg[:, :, 0] = image_rgb[:, :, 0]  # Red channel
            image_rg[:, :, 1] = image_rgb[:, :, 1]  # Green channel
            
            # Convert to PIL Image for transforms
            image_pil = Image.fromarray(image_rg)
            
            if self.transform:
                image_tensor = self.transform(image_pil)
            else:
                # Default conversion if no transform
                image_tensor = torch.from_numpy(image_rg).permute(2, 0, 1).float() / 255.0
                
        except Exception as e:
            print(f"Error loading image {img_name}: {e}")
            # Return a dummy tensor in case of error
            image_tensor = torch.zeros((2, 512, 512), dtype=torch.float32)
            
        return image_tensor, img_name

class DRRGPVT2Classifier(nn.Module):
    """
    Diabetic Retinopathy Classifier for RG (Red-Green) channels using PVT2 transformer
    """
    def __init__(self, num_classes=5, pretrained=False, dropout_rate=0.5, model_name='pvt_v2_b2'):
        super(DRRGPVT2Classifier, self).__init__()
        
        # Use PVT2 from timm as backbone
        self.backbone = timm.create_model(
            model_name, 
            pretrained=pretrained, 
            num_classes=0,  # We'll add our own classifier
            in_chans=2  # RG has 2 channels
        )
        
        # Get the number of features from the backbone
        self.num_features = self.backbone.num_features
        
        # Replace the classifier head
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(self.num_features, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate/2),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate/4),
            nn.Linear(256, num_classes)
        )
        
    def forward(self, x):
        # Forward through PVT2 backbone
        features = self.backbone(x)
        
        # Classify
        output = self.classifier(features)
        return output

def get_rg_test_transforms(rg_mean, rg_std):
    """
    Define preprocessing transforms for test images (no augmentation)
    Uses RG-specific normalization
    """
    test_transforms = transforms.Compose([
        transforms.Resize((512, 512)),  # PVT2 uses 512x512
        transforms.ToTensor(),
        transforms.Normalize(mean=rg_mean, std=rg_std)
    ])
    
    return test_transforms

def safe_torch_load(model_path, device):
    """
    Safely load torch model with compatibility for PyTorch 2.6+
    """
    try:
        # First try with weights_only=True (safe mode)
        checkpoint = torch.load(model_path, map_location=device, weights_only=True)
        print("Model loaded with weights_only=True (safe mode)")
        return checkpoint
    except Exception as e:
        print(f"Safe loading failed: {e}")
        print("Trying with weights_only=False (requires trust in the model source)")
        
        # If safe mode fails, try with weights_only=False
        try:
            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
            print("Model loaded with weights_only=False")
            return checkpoint
        except Exception as e2:
            print(f"Loading with weights_only=False also failed: {e2}")
            
            # Last resort: try using the safe_globals context manager
            try:
                import numpy.core.multiarray
                with torch.serialization.safe_globals([numpy.core.multiarray.scalar]):
                    checkpoint = torch.load(model_path, map_location=device)
                print("Model loaded with safe_globals context manager")
                return checkpoint
            except Exception as e3:
                print(f"All loading methods failed: {e3}")
                raise

def load_trained_model(model_path, num_classes=5, dropout_rate=0.5):
    """
    Load the pre-trained RG PVT2 model
    """
    print(f"Loading model from: {model_path}")
    
    # Initialize model architecture WITHOUT downloading pretrained weights
    model = DRRGPVT2Classifier(
        num_classes=num_classes,
        dropout_rate=dropout_rate,
        pretrained=False,
        model_name='pvt_v2_b2'
    )
    
    # Load trained weights using safe loading
    checkpoint = safe_torch_load(model_path, device)
    
    # Handle different checkpoint formats
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print("Loaded model from model_state_dict")
    elif 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'])
        print("Loaded model from state_dict")
    else:
        # Try direct loading
        try:
            model.load_state_dict(checkpoint)
            print("Loaded model from direct state_dict")
        except:
            # If all else fails, try to find the state dict in the checkpoint
            for key in checkpoint.keys():
                if 'state_dict' in key or 'model' in key:
                    print(f"Trying key: {key}")
                    try:
                        model.load_state_dict(checkpoint[key])
                        print(f"Loaded model from key: {key}")
                        break
                    except:
                        continue
        
    model.to(device)
    model.eval()
    
    print("Model loaded successfully!")
    return model

def load_normalization_stats(stats_path):
    """
    Load RG normalization statistics from training
    """
    try:
        with open(stats_path, 'r') as f:
            stats = json.load(f)
        rg_mean = stats['rg_mean']
        rg_std = stats['rg_std']
        print(f"Loaded RG normalization stats: mean={rg_mean}, std={rg_std}")
        return rg_mean, rg_std
    except FileNotFoundError:
        print(f"Warning: Normalization stats file not found at {stats_path}")
        print("Using default RG normalization")
        return [0.5, 0.5], [0.5, 0.5]

def generate_predictions(model, test_loader, use_tta=False):
    """
    Generate predictions for test set
    
    Args:
        model: Trained model
        test_loader: DataLoader for test set
        use_tta: Whether to use Test Time Augmentation (TTA)
    """
    model.eval()
    predictions = []
    image_names = []
    all_probabilities = []
    
    print("Generating predictions...")
    
    with torch.no_grad():
        for data, names in tqdm(test_loader, desc="Processing test images"):
            data = data.to(device)
            
            if use_tta:
                # Test Time Augmentation
                outputs = model(data)
                
                # Horizontal flip
                data_hflip = torch.flip(data, dims=[3])
                outputs += model(data_hflip)
                
                # Vertical flip
                data_vflip = torch.flip(data, dims=[2])
                outputs += model(data_vflip)
                
                # Average the predictions
                outputs = outputs / 3.0
            else:
                outputs = model(data)
            
            # Convert logits to probabilities
            probs = torch.softmax(outputs, dim=1)
            
            # Get predicted classes
            preds = probs.argmax(dim=1)
            
            predictions.extend(preds.cpu().numpy())
            image_names.extend(names)
            all_probabilities.extend(probs.cpu().numpy())
    
    return predictions, image_names, np.array(all_probabilities)

def create_submission_file(predictions, image_names, output_file='rg_pvt2_submission.csv'):
    """
    Create submission file in Kaggle format
    """
    submission_df = pd.DataFrame({
        'id_code': image_names,
        'diagnosis': predictions
    })
    
    # Sort by id_code to ensure consistent ordering
    submission_df = submission_df.sort_values('id_code')
    
    # Save to CSV
    submission_df.to_csv(output_file, index=False)
    
    print(f"Submission file saved: {output_file}")
    print(f"Total predictions: {len(submission_df)}")
    
    # Print prediction distribution
    print("\nPrediction distribution:")
    label_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative']
    for i, label in enumerate(label_names):
        count = (submission_df['diagnosis'] == i).sum()
        percentage = count / len(submission_df) * 100
        print(f"  Class {i} ({label}): {count} images ({percentage:.1f}%)")
    
    return submission_df

def main():
    """
    Main inference pipeline for RG PVT2 model
    """
    # Configuration - UPDATE THESE PATHS FOR YOUR KAGGLE NOTEBOOK
    config = {
        'model_path': '/kaggle/input/rg-pvt2-training/best_rg_pvt2_model.pth',  # Update this path
        'normalization_stats_path': '/kaggle/input/rg-pvt2-training/rg_pvt2_normalization_stats.json',  # Update this path
        'test_csv': '/kaggle/input/aptos2019-blindness-detection/test.csv',
        'test_img_dir': '/kaggle/input/aptos2019-blindness-detection/test_images',
        'batch_size': 16,
        'num_classes': 5,
        'dropout_rate': 0.5,
        'use_tta': True,  # Use Test Time Augmentation for better results
        'output_file': 'submission.csv'
    }
    
    print("="*70)
    print("DIABETIC RETINOPATHY RG (RED-GREEN) PVT2 TRANSFORMER MODEL - INFERENCE")
    print("="*70)
    print("Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print("="*70)
    
    # Check if test files exist
    if not os.path.exists(config['test_csv']):
        print(f"Error: Test CSV file not found at {config['test_csv']}")
        return
    
    if not os.path.exists(config['test_img_dir']):
        print(f"Error: Test images directory not found at {config['test_img_dir']}")
        return
    
    if not os.path.exists(config['model_path']):
        print(f"Error: Model file not found at {config['model_path']}")
        print("Please make sure the model file exists or update the model_path in the config.")
        # Try to find the model in common locations
        possible_paths = [
            '/kaggle/input/best-rg-pvt2-model/best_rg_pvt2_model.pth',
            '/kaggle/working/best_rg_pvt2_model.pth',
            'best_rg_pvt2_model.pth'
        ]
        for path in possible_paths:
            if os.path.exists(path):
                config['model_path'] = path
                print(f"Found model at: {path}")
                break
        else:
            return
    
    # Load normalization statistics
    if os.path.exists(config['normalization_stats_path']):
        rg_mean, rg_std = load_normalization_stats(config['normalization_stats_path'])
    else:
        # Try to find the stats file in common locations
        possible_stats_paths = [
            '/kaggle/input/best-rg-pvt2-model/rg_pvt2_normalization_stats.json',
            '/kaggle/working/rg_pvt2_normalization_stats.json',
            'rg_pvt2_normalization_stats.json',
            os.path.join(os.path.dirname(config['model_path']), 'rg_pvt2_normalization_stats.json')
        ]
        for path in possible_stats_paths:
            if os.path.exists(path):
                rg_mean, rg_std = load_normalization_stats(path)
                break
        else:
            print("Warning: Using default RG normalization statistics")
            rg_mean, rg_std = [0.5, 0.5], [0.5, 0.5]
    
    # Load trained model
    model = load_trained_model(
        config['model_path'], 
        config['num_classes'], 
        config['dropout_rate']
    )
    
    # Get RG-specific test transforms
    test_transforms = get_rg_test_transforms(rg_mean, rg_std)
    
    # Load test dataset
    print("\nLoading test data...")
    test_dataset = DiabeticRetinopathyRGDataset(
        csv_file=config['test_csv'],
        img_dir=config['test_img_dir'],
        transform=test_transforms
    )
    
    # Create test data loader
    test_loader = DataLoader(
        test_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=4,
        pin_memory=True
    )
    
    # Generate predictions
    print(f"\nGenerating predictions with TTA: {config['use_tta']}")
    predictions, image_names, probabilities = generate_predictions(
        model, 
        test_loader, 
        use_tta=config['use_tta']
    )
    
    # Create submission file
    print("\nCreating submission file...")
    submission_df = create_submission_file(
        predictions, 
        image_names, 
        config['output_file']
    )
    
    # Calculate confidence statistics
    max_probs = np.max(probabilities, axis=1)
    print(f"\nConfidence statistics:")
    print(f"  Mean confidence: {np.mean(max_probs):.4f}")
    print(f"  Std confidence: {np.std(max_probs):.4f}")
    print(f"  Min confidence: {np.min(max_probs):.4f}")
    print(f"  Max confidence: {np.max(max_probs):.4f}")
    
    # Show samples with lowest confidence
    low_confidence_indices = np.argsort(max_probs)[:5]
    print(f"\n5 predictions with lowest confidence:")
    for i, idx in enumerate(low_confidence_indices):
        print(f"  {i+1}. {image_names[idx]}: Class {predictions[idx]} (confidence: {max_probs[idx]:.4f})")
    
    print("\n" + "="*70)
    print("RG PVT2 INFERENCE COMPLETED SUCCESSFULLY!")
    print("="*70)
    print(f"Submission file: {config['output_file']}")
    print(f"Total test images processed: {len(predictions)}")
    print(f"RG normalization used: mean={rg_mean}, std={rg_std}")
    print("="*70)
    
    return submission_df

if __name__ == "__main__":
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # Run inference
    submission_df = main()
