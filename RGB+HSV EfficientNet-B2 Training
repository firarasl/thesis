import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import json
from datetime import datetime
import random
warnings.filterwarnings('ignore')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

def crop_black_background(image, threshold=10):
    """
    Crop black background from retinal images
    """
    # Convert to grayscale for processing
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    else:
        gray = image
    
    # Create mask for non-black pixels
    mask = gray > threshold
    
    # Find bounding box of non-black region
    coords = np.argwhere(mask)
    if len(coords) == 0:
        return image  # Return original if no non-black pixels found
    
    y0, x0 = coords.min(axis=0)
    y1, x1 = coords.max(axis=0) + 1
    
    # Crop the image
    cropped = image[y0:y1, x0:x1]
    
    return cropped

def apply_perspective_transform(image, max_shift=0.1):
    """
    Apply random perspective transformation
    """
    h, w = image.shape[:2]
    
    # Random shift amount
    shift_x = random.uniform(-max_shift, max_shift) * w
    shift_y = random.uniform(-max_shift, max_shift) * h
    
    # Define source and destination points
    src_points = np.float32([[0, 0], [w, 0], [w, h], [0, h]])
    dst_points = np.float32([
        [shift_x, shift_y],
        [w - shift_x, shift_y],
        [w - shift_x, h - shift_y],
        [shift_x, h - shift_y]
    ])
    
    # Get perspective transform matrix
    matrix = cv2.getPerspectiveTransform(src_points, dst_points)
    
    # Apply transformation
    transformed = cv2.warpPerspective(image, matrix, (w, h))
    
    return transformed

class CustomAugmentations:
    """
    Custom augmentation pipeline based on the tips
    """
    def __init__(self, is_training=True):
        self.is_training = is_training
    
    def __call__(self, image):
        if not self.is_training:
            return image
        
        # Convert PIL to numpy if needed
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        # Apply augmentations with certain probabilities
        if random.random() < 0.5:
            # Horizontal flip
            image = cv2.flip(image, 1)
        
        if random.random() < 0.5:
            # Vertical flip
            image = cv2.flip(image, 0)
        
        if random.random() < 0.7:
            # Random rotation (0-360 degrees)
            angle = random.uniform(0, 360)
            h, w = image.shape[:2]
            center = (w // 2, h // 2)
            matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            image = cv2.warpAffine(image, matrix, (w, h))
        
        if random.random() < 0.5:
            # Zoom (1.0 to 1.35)
            zoom_factor = random.uniform(1.0, 1.35)
            h, w = image.shape[:2]
            new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)
            
            # Resize image
            resized = cv2.resize(image, (new_w, new_h))
            
            # Crop or pad to original size
            if zoom_factor > 1.0:
                # Crop center
                start_x = (new_w - w) // 2
                start_y = (new_h - h) // 2
                image = resized[start_y:start_y+h, start_x:start_x+w]
            else:
                # Pad
                pad_x = (w - new_w) // 2
                pad_y = (h - new_h) // 2
                image = cv2.copyMakeBorder(resized, pad_y, pad_y, pad_x, pad_x, cv2.BORDER_CONSTANT)
        
        if random.random() < 0.3:
            # Apply perspective transformation
            image = apply_perspective_transform(image)
        
        return image

class DiabeticRetinopathyRGBHSVDataset(Dataset):
    """
    Enhanced Dataset for Diabetic Retinopathy with RGB + HSV fusion and improved preprocessing
    """
    def __init__(self, csv_file, img_dir, transform=None, is_test=False, image_size=384):
        """
        Args:
            csv_file (string): Path to csv with image names and labels
            img_dir (string): Directory with all images
            transform (callable, optional): Transform to be applied on images
            is_test (bool): Whether this is test dataset (no labels)
            image_size (int): Target image size (384x384)
        """
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        self.is_test = is_test
        self.image_size = image_size
        
        # Print dataset statistics
        if not is_test:
            print(f"Dataset loaded: {len(self.data)} images")
            label_counts = self.data.iloc[:, 1].value_counts().sort_index()
            print("Label distribution:")
            for label, count in label_counts.items():
                print(f"  Class {label}: {count} images ({count/len(self.data)*100:.1f}%)")
        
    def __len__(self):
        return len(self.data)
    
    def preprocess_image(self, image_path):
        """
        Enhanced preprocessing with black background cropping
        """
        # Load image
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Could not load image: {image_path}")
        
        # Convert BGR to RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Crop black background
        image = crop_black_background(image, threshold=10)
        
        # Resize to target size
        image = cv2.resize(image, (self.image_size, self.image_size))
        
        return image
    
    def rgb_to_hsv_tensor(self, rgb_image):
        """
        Convert RGB image to HSV
        """
        # Convert RGB to HSV
        hsv_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)
        return hsv_image
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name
        img_name = self.data.iloc[idx, 0]
        
        # Try different extensions
        img_extensions = ['.jpeg', '.jpg', '.png']
        img_path = None
        
        for ext in img_extensions:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension")
        
        # Load and preprocess image
        rgb_image = self.preprocess_image(img_path)
        
        # Convert to HSV
        hsv_image = self.rgb_to_hsv_tensor(rgb_image)
        
        # Apply custom augmentations if transform is CustomAugmentations
        if self.transform and hasattr(self.transform, '__call__'):
            if isinstance(self.transform, CustomAugmentations):
                # Apply same augmentation to both RGB and HSV
                # Save random state to ensure same augmentation
                random_state = random.getstate()
                
                rgb_image = self.transform(rgb_image)
                
                # Restore random state and apply same augmentation to HSV
                random.setstate(random_state)
                hsv_image = self.transform(hsv_image)
            else:
                # Standard transforms - handle RGB and HSV separately
                rgb_pil = Image.fromarray(rgb_image)
                hsv_pil = Image.fromarray(hsv_image)
                
                # Apply transforms to each separately
                rgb_tensor = self.transform(rgb_pil)
                hsv_tensor = self.transform(hsv_pil)
                
                # Return as tuple for dual-stream processing
                if not self.is_test:
                    label = int(self.data.iloc[idx, 1])
                    return (rgb_tensor, hsv_tensor), label
                else:
                    return (rgb_tensor, hsv_tensor), img_name
        
        # Handle case where CustomAugmentations returned numpy array
        if isinstance(rgb_image, np.ndarray) and isinstance(hsv_image, np.ndarray):
            # Convert to tensors
            rgb_tensor = torch.from_numpy(rgb_image.transpose(2, 0, 1)).float() / 255.0
            hsv_tensor = torch.from_numpy(hsv_image.transpose(2, 0, 1)).float() / 255.0
            
            # Normalize both with ImageNet stats
            rgb_tensor = transforms.Normalize(
                mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225]
            )(rgb_tensor)
            
            hsv_tensor = transforms.Normalize(
                mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225]
            )(hsv_tensor)
            
            # Return as tuple for dual-stream processing
            if not self.is_test:
                label = int(self.data.iloc[idx, 1])
                return (rgb_tensor, hsv_tensor), label
            else:
                return (rgb_tensor, hsv_tensor), img_name
        
        # Fallback: convert to tensors without augmentation
        rgb_tensor = torch.from_numpy(rgb_image.transpose(2, 0, 1)).float() / 255.0
        hsv_tensor = torch.from_numpy(hsv_image.transpose(2, 0, 1)).float() / 255.0
        
        # Normalize
        rgb_tensor = transforms.Normalize(
            mean=[0.485, 0.456, 0.406], 
            std=[0.229, 0.224, 0.225]
        )(rgb_tensor)
        
        hsv_tensor = transforms.Normalize(
            mean=[0.485, 0.456, 0.406], 
            std=[0.229, 0.224, 0.225]
        )(hsv_tensor)
        
        if not self.is_test:
            label = int(self.data.iloc[idx, 1])
            return (rgb_tensor, hsv_tensor), label
        else:
            return (rgb_tensor, hsv_tensor), img_name

class DualStreamEfficientNetFusion(nn.Module):
    """
    Dual-stream architecture using torchvision's EfficientNet-B2 for both RGB and HSV streams
    """
    def __init__(self, num_classes=5, dropout_rate=0.5):
        super(DualStreamEfficientNetFusion, self).__init__()
        
        # RGB stream - EfficientNet-B2
        self.rgb_backbone = models.efficientnet_b2(pretrained=True)
        rgb_features = self.rgb_backbone.classifier[1].in_features
        self.rgb_backbone.classifier = nn.Identity()  # Remove final classifier
        
        # HSV stream - EfficientNet-B2
        self.hsv_backbone = models.efficientnet_b2(pretrained=True)
        hsv_features = self.hsv_backbone.classifier[1].in_features
        self.hsv_backbone.classifier = nn.Identity()  # Remove final classifier
        
        # Fusion layer
        combined_features = rgb_features + hsv_features  # 1408 + 1408 = 2816
        
        # Final classifier with enhanced regularization
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(combined_features, 1024),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate/2),
            nn.Linear(1024, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate/2),
            nn.Linear(512, num_classes)
        )
        
        # Initialize new layers
        for m in self.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # x is a tuple of (rgb_tensor, hsv_tensor)
        rgb, hsv = x
        
        # Extract features from both streams
        rgb_features = self.rgb_backbone(rgb)
        hsv_features = self.hsv_backbone(hsv)
        
        # Concatenate features
        combined_features = torch.cat([rgb_features, hsv_features], dim=1)
        
        # Final classification
        output = self.classifier(combined_features)
        
        return output

class EnhancedTransforms:
    """
    Enhanced transforms with improved preprocessing for dual-stream architecture
    """
    def __init__(self, size=384, is_training=True):
        self.size = size
        self.is_training = is_training
        
        if is_training:
            # Use custom augmentations
            self.augmentation = CustomAugmentations(is_training=True)
        else:
            # No augmentation for validation/test
            self.augmentation = CustomAugmentations(is_training=False)
    
    def __call__(self, image):
        """
        Apply transforms to image
        """
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        # Apply augmentations
        image = self.augmentation(image)
        
        # Convert to PIL for final processing
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        
        # Convert to tensor and normalize
        image = transforms.ToTensor()(image)
        
        # Apply normalization
        image = transforms.Normalize(
            mean=[0.485, 0.456, 0.406], 
            std=[0.229, 0.224, 0.225]
        )(image)
        
        return image

def get_enhanced_transforms():
    """
    Get enhanced transforms for training and validation
    """
    train_transforms = EnhancedTransforms(size=384, is_training=True)
    val_transforms = EnhancedTransforms(size=384, is_training=False)
    
    return train_transforms, val_transforms

def quadratic_weighted_kappa(y_true, y_pred):
    """
    Calculate Quadratic Weighted Kappa (QWK) score
    """
    return cohen_kappa_score(y_true, y_pred, weights='quadratic')

def train_dual_stream_model(model, train_loader, val_loader, num_epochs=40, learning_rate=1e-4):
    """
    Train the dual-stream RGB+HSV fusion model
    """
    # Loss function with label smoothing
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    # Optimizer with different learning rates for different parts
    rgb_backbone_params = list(model.rgb_backbone.parameters())
    hsv_backbone_params = list(model.hsv_backbone.parameters())
    classifier_params = list(model.classifier.parameters())
    
    optimizer = optim.AdamW([
        {'params': rgb_backbone_params, 'lr': learning_rate * 0.1, 'weight_decay': 1e-4},
        {'params': hsv_backbone_params, 'lr': learning_rate * 0.1, 'weight_decay': 1e-4},
        {'params': classifier_params, 'lr': learning_rate, 'weight_decay': 1e-4}
    ])
    
    # Enhanced learning rate scheduler
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, 
        max_lr=learning_rate,
        steps_per_epoch=len(train_loader),
        epochs=num_epochs,
        pct_start=0.3,
        anneal_strategy='cos'
    )
    
    # Training history
    history = {
        'train_losses': [],
        'val_losses': [],
        'val_kappas': [],
        'val_accuracies': [],
        'learning_rates': []
    }
    
    best_kappa = 0.0
    best_model_state = None
    patience_counter = 0
    patience = 15
    
    print("Starting Dual-Stream EfficientNet-B2 RGB+HSV Fusion Model Training...")
    print(f"Training on {len(train_loader.dataset)} samples")
    print(f"Validating on {len(val_loader.dataset)} samples")
    print(f"Image size: 384x384")
    print(f"Architecture: Dual-stream EfficientNet-B2 (RGB + HSV)")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')
        
        for batch_idx, (data, target) in enumerate(train_pbar):
            # data is a tuple of (rgb_tensor, hsv_tensor)
            rgb_data, hsv_data = data
            rgb_data = rgb_data.to(device)
            hsv_data = hsv_data.to(device)
            target = target.to(device)
            
            # Skip batches with size 1 to avoid BatchNorm issues
            if rgb_data.size(0) == 1:
                continue
            
            # Verify input shapes
            if batch_idx == 0:
                print(f"RGB input shape: {rgb_data.shape}")
                print(f"HSV input shape: {hsv_data.shape}")
            
            optimizer.zero_grad()
            output = model((rgb_data, hsv_data))
            loss = criterion(output, target)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            train_correct += pred.eq(target.view_as(pred)).sum().item()
            train_total += target.size(0)
            
            # Update progress bar
            current_acc = 100. * train_correct / train_total
            train_pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_acc:.2f}%',
                'LR': f'{scheduler.get_last_lr()[0]:.2e}'
            })
        
        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = 100. * train_correct / train_total
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')
            for data, target in val_pbar:
                rgb_data, hsv_data = data
                rgb_data = rgb_data.to(device)
                hsv_data = hsv_data.to(device)
                target = target.to(device)
                
                if rgb_data.size(0) == 1:
                    continue
                    
                output = model((rgb_data, hsv_data))
                loss = criterion(output, target)
                val_loss += loss.item()
                
                pred = output.argmax(dim=1, keepdim=True)
                val_correct += pred.eq(target.view_as(pred)).sum().item()
                val_total += target.size(0)
                
                all_preds.extend(pred.cpu().numpy().flatten())
                all_targets.extend(target.cpu().numpy())
                
                current_acc = 100. * val_correct / val_total
                val_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })
        
        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100. * val_correct / val_total
        val_kappa = quadratic_weighted_kappa(all_targets, all_preds)
        
        current_lr = scheduler.get_last_lr()[0]
        
        # Store history
        history['train_losses'].append(avg_train_loss)
        history['val_losses'].append(avg_val_loss)
        history['val_kappas'].append(val_kappa)
        history['val_accuracies'].append(val_accuracy)
        history['learning_rates'].append(current_lr)
        
        # Print epoch results
        print(f'\nEpoch {epoch+1}/{num_epochs}:')
        print(f'  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%')
        print(f'  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%')
        print(f'  Val QWK: {val_kappa:.4f}')
        print(f'  Learning Rate: {current_lr:.8f}')
        
        # Save best model
        if val_kappa > best_kappa:
            best_kappa = val_kappa
            best_model_state = model.state_dict().copy()
            torch.save(best_model_state, 'best_dual_stream_efficientnet_model.pth')
            patience_counter = 0
            print(f'  âœ… New best model saved! QWK: {best_kappa:.4f}')
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f'\nEarly stopping triggered after {patience} epochs without improvement.')
            break
            
        print('-' * 70)
    
    # Load best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    history['best_kappa'] = best_kappa
    history['total_epochs'] = epoch + 1
    
    return model, history

def main():
    """
    Main training pipeline for dual-stream RGB+HSV fusion model
    """
    # Configuration
    config = {
        'train_csv': '/kaggle/input/aptos2019-blindness-detection/train.csv',
        'test_csv': '/kaggle/input/aptos2019-blindness-detection/test.csv',
        'train_img_dir': '/kaggle/input/aptos2019-blindness-detection/train_images',
        'test_img_dir': '/kaggle/input/aptos2019-blindness-detection/test_images',
        'batch_size': 16,
        'num_epochs': 40,
        'learning_rate': 1e-4,
        'num_classes': 5,
        'dropout_rate': 0.5,
        'image_size': 384
    }
    
    print("="*70)
    print("DUAL-STREAM EFFICIENTNET-B2 DIABETIC RETINOPATHY RGB+HSV FUSION MODEL")
    print("="*70)
    print("Architecture Features:")
    print("  - Dual-stream EfficientNet-B2 (RGB + HSV)")
    print("  - Pretrained weights from ImageNet")
    print("  - Feature-level fusion (2816 features)")
    print("  - Black background cropping")
    print("  - Enhanced augmentations")
    print("  - Label smoothing + OneCycleLR")
    print("="*70)
    
    # Get enhanced transforms
    train_transforms, val_transforms = get_enhanced_transforms()
    
    # Load training data
    print("\nLoading dual-stream RGB+HSV training data...")
    full_train_dataset = DiabeticRetinopathyRGBHSVDataset(
        csv_file=config['train_csv'],
        img_dir=config['train_img_dir'],
        transform=train_transforms,
        image_size=config['image_size']
    )
    
    # Split into train and validation
    train_size = int(0.8 * len(full_train_dataset))
    val_size = len(full_train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_train_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        num_workers=4,
        pin_memory=True,
        drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=4,
        pin_memory=True,
        drop_last=False
    )
    
    # Initialize dual-stream model
    print("\nInitializing dual-stream EfficientNet-B2 RGB+HSV fusion model...")
    model = DualStreamEfficientNetFusion(
        num_classes=config['num_classes'],
        dropout_rate=config['dropout_rate']
    )
    model.to(device)
    
    # Train dual-stream model
    print("\nStarting dual-stream EfficientNet-B2 RGB+HSV fusion model training...")
    model, history = train_dual_stream_model(
        model, 
        train_loader, 
        val_loader, 
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate']
    )
    
    print(f"\nTraining completed!")
    print(f"Best QWK: {history['best_kappa']:.4f}")
    print(f"Total epochs: {history['total_epochs']}")
    
    return model, history

if __name__ == "__main__":
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # Run dual-stream RGB+HSV fusion experiment
    model, history = main()
