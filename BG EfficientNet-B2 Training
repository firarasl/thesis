import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import os
from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report
from sklearn.model_selection import StratifiedKFold
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import json
from datetime import datetime
import random
warnings.filterwarnings('ignore')

# Set seeds for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

def calculate_gb_normalization_stats(csv_file, img_dir, sample_size=500):
    """
    Calculate proper normalization statistics for GB channels from actual data
    """
    print("Calculating GB channel normalization statistics...")
    data = pd.read_csv(csv_file)
    
    # Sample images for statistics calculation
    sample_indices = np.random.choice(len(data), min(sample_size, len(data)), replace=False)
    
    all_pixels_g = []
    all_pixels_b = []
    
    for idx in tqdm(sample_indices, desc="Computing stats"):
        img_name = data.iloc[idx, 0]
        
        # Try different extensions
        img_path = None
        for ext in ['.jpeg', '.jpg', '.png']:
            potential_path = os.path.join(img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
        
        if img_path is None:
            continue
            
        try:
            # Load image
            image = cv2.imread(img_path)
            if image is None:
                continue
                
            # Convert BGR to RGB
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Resize to standard size for consistent statistics
            image_resized = cv2.resize(image_rgb, (512, 512))
            
            # Extract Green and Blue channels and normalize to [0,1]
            green_channel = image_resized[:, :, 1].flatten() / 255.0
            blue_channel = image_resized[:, :, 2].flatten() / 255.0
            
            # Sample pixels to avoid memory issues
            sample_pixels = min(1000, len(green_channel))
            indices = np.random.choice(len(green_channel), sample_pixels, replace=False)
            
            all_pixels_g.extend(green_channel[indices])
            all_pixels_b.extend(blue_channel[indices])
            
        except Exception as e:
            print(f"Error processing {img_name}: {e}")
            continue
    
    if not all_pixels_g or not all_pixels_b:
        print("Warning: No valid images found, using default normalization")
        return [0.5, 0.5], [0.5, 0.5]
    
    # Calculate statistics
    mean_g = np.mean(all_pixels_g)
    mean_b = np.mean(all_pixels_b)
    std_g = np.std(all_pixels_g)
    std_b = np.std(all_pixels_b)
    
    # Ensure std is not too small
    std_g = max(std_g, 0.1)
    std_b = max(std_b, 0.1)
    
    print(f"GB Channel Statistics (from {len(sample_indices)} images):")
    print(f"  Green Channel - Mean: {mean_g:.4f}, Std: {std_g:.4f}")
    print(f"  Blue Channel - Mean: {mean_b:.4f}, Std: {std_b:.4f}")
    
    return [mean_g, mean_b], [std_g, std_b]

class DiabeticRetinopathyGBDataset(Dataset):
    """
    Custom Dataset for Diabetic Retinopathy with GB (Green-Blue) channels only
    """
    def __init__(self, csv_file, img_dir, transform=None, is_test=False):
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        self.is_test = is_test
        
        # Print dataset info
        if not is_test:
            print(f"GB Dataset loaded: {len(self.data)} images")
            label_counts = self.data.iloc[:, 1].value_counts().sort_index()
            print("Class distribution:")
            for label, count in label_counts.items():
                print(f"  Class {label}: {count} images ({count/len(self.data)*100:.1f}%)")
        else:
            print(f"Test dataset loaded: {len(self.data)} images")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        # Get image name (handle both string and numeric names)
        img_name = str(self.data.iloc[idx, 0])
        
        # Try different extensions
        img_path = None
        for ext in ['.jpeg', '.jpg', '.png']:
            potential_path = os.path.join(self.img_dir, f"{img_name}{ext}")
            if os.path.exists(potential_path):
                img_path = potential_path
                break
                
        if img_path is None:
            raise FileNotFoundError(f"Image {img_name} not found with any extension")
        
        try:
            # Load image
            image = cv2.imread(img_path)
            if image is None:
                raise ValueError(f"Could not load image: {img_path}")
            
            # Convert BGR to RGB
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Extract Green and Blue channels only
            image_gb = np.zeros((image_rgb.shape[0], image_rgb.shape[1], 2), dtype=np.uint8)
            image_gb[:, :, 0] = image_rgb[:, :, 1]  # Green channel
            image_gb[:, :, 1] = image_rgb[:, :, 2]  # Blue channel
            
            # Convert to tensor format directly
            image_tensor = torch.from_numpy(image_gb).permute(2, 0, 1).float() / 255.0
            
            if self.transform:
                # Apply transforms that work with tensors
                image_tensor = self.transform(image_tensor)
                
        except Exception as e:
            print(f"Error loading image {img_name}: {e}")
            # Return a dummy tensor in case of error
            image_tensor = torch.zeros((2, 512, 512), dtype=torch.float32)
            
        if not self.is_test:
            label = int(self.data.iloc[idx, 1])
            return image_tensor, label
        else:
            return image_tensor, img_name

class GBTransforms:
    """
    Custom transforms for GB (2-channel) images
    """
    def __init__(self, size=(512, 512), mean=None, std=None, is_training=False):
        self.size = size
        self.mean = mean or [0.5, 0.5]
        self.std = std or [0.5, 0.5]
        self.is_training = is_training
        
    def __call__(self, tensor):
        # tensor shape: (2, H, W)
        
        # Resize
        tensor = torch.nn.functional.interpolate(
            tensor.unsqueeze(0), 
            size=self.size, 
            mode='bilinear', 
            align_corners=False
        ).squeeze(0)
        
        # Data augmentation for training
        if self.is_training:
            # Random horizontal flip
            if torch.rand(1) > 0.5:
                tensor = torch.flip(tensor, [-1])
                
            # Random vertical flip
            if torch.rand(1) > 0.5:
                tensor = torch.flip(tensor, [-2])
                
            # Random rotation (simplified)
            if torch.rand(1) > 0.7:
                # 90-degree rotations only for simplicity
                k = torch.randint(0, 4, (1,)).item()
                tensor = torch.rot90(tensor, k, [-2, -1])
        
        # Normalize - CRITICAL: Apply channel-specific normalization
        for i in range(2):
            tensor[i] = (tensor[i] - self.mean[i]) / self.std[i]
            
        return tensor

class DREfficientNetB2GBClassifier(nn.Module):
    """
    Diabetic Retinopathy Classifier using EfficientNet-B2 for GB (Green-Blue) channels
    """
    def __init__(self, num_classes=5, pretrained=True, dropout_rate=0.4):
        super(DREfficientNetB2GBClassifier, self).__init__()
        
        # Use EfficientNet-B2 as backbone
        self.backbone = models.efficientnet_b2(pretrained=pretrained)
        
        # Store original first conv layer for weight adaptation
        original_conv = self.backbone.features[0][0]
        
        # Modify first conv layer for 2 input channels (GB)
        self.backbone.features[0][0] = nn.Conv2d(
            2, original_conv.out_channels, 
            kernel_size=original_conv.kernel_size,
            stride=original_conv.stride,
            padding=original_conv.padding,
            bias=original_conv.bias is not None
        )
        
        # Initialize new conv1 weights
        if pretrained:
            with torch.no_grad():
                # Use Green and Blue weights from pretrained model
                # Green channel (index 1 in RGB) -> GB channel 0
                self.backbone.features[0][0].weight[:, 0:1, :, :] = original_conv.weight[:, 1:2, :, :]
                # Blue channel (index 2 in RGB) -> GB channel 1  
                self.backbone.features[0][0].weight[:, 1:2, :, :] = original_conv.weight[:, 2:3, :, :]
                
                # Copy bias if exists
                if original_conv.bias is not None:
                    self.backbone.features[0][0].bias = nn.Parameter(original_conv.bias.clone())
            print("Initialized GB conv1 weights from pretrained EfficientNet-B2 weights (Green and Blue channels)")
        else:
            # Initialize with He initialization
            nn.init.kaiming_normal_(self.backbone.features[0][0].weight, mode='fan_out', nonlinearity='relu')
            if self.backbone.features[0][0].bias is not None:
                nn.init.constant_(self.backbone.features[0][0].bias, 0)
        
        # Get the number of features in the classifier
        num_features = self.backbone.classifier[1].in_features
        
        # Replace final layer with custom classifier optimized for EfficientNet
        self.backbone.classifier = nn.Sequential(
            nn.Dropout(p=dropout_rate),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate/2),
            nn.Linear(512, num_classes)
        )
        
        # Initialize weights for new layers
        for m in self.backbone.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        return self.backbone(x)

def quadratic_weighted_kappa(y_true, y_pred):
    """Calculate Quadratic Weighted Kappa score"""
    return cohen_kappa_score(y_true, y_pred, weights='quadratic')

def train_gb_effnet_model(model, train_loader, val_loader, num_epochs=50, learning_rate=2e-4, model_name="gb_effnet"):
    """
    Train GB channel EfficientNet-B2 model with improved training loop
    """
    # Loss function with class weights for imbalanced data
    criterion = nn.CrossEntropyLoss()
    
    # Optimizer with weight decay - optimized for EfficientNet
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=learning_rate, 
        weight_decay=1e-4,
        betas=(0.9, 0.999)
    )
    
    # Learning rate scheduler
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )
    
    # Training history
    history = {
        'train_losses': [],
        'val_losses': [],
        'val_kappas': [],
        'val_accuracies': [],
        'learning_rates': []
    }
    
    best_kappa = 0.0
    best_model_state = None
    patience_counter = 0
    patience = 15
    
    print(f"\n{'='*70}")
    print("STARTING GB CHANNEL EFFICIENTNET-B2 MODEL TRAINING")
    print(f"{'='*70}")
    print(f"Training samples: {len(train_loader.dataset)}")
    print(f"Validation samples: {len(val_loader.dataset)}")
    print(f"Batch size: {train_loader.batch_size}")
    print(f"Learning rate: {learning_rate}")
    print(f"Model: EfficientNet-B2 with GB channels")
    print(f"{'='*70}")
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [GB-EffNet Train]')
        
        for batch_idx, (data, target) in enumerate(train_pbar):
            data, target = data.to(device), target.to(device)
            
            # Check for NaN or invalid data
            if torch.isnan(data).any() or torch.isinf(data).any():
                print(f"Warning: NaN/Inf detected in input data at batch {batch_idx}")
                continue
            
            optimizer.zero_grad()
            
            try:
                output = model(data)
                loss = criterion(output, target)
                
                # Check for NaN loss
                if torch.isnan(loss):
                    print(f"Warning: NaN loss at batch {batch_idx}")
                    continue
                    
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                train_correct += pred.eq(target.view_as(pred)).sum().item()
                train_total += target.size(0)
                
                current_acc = 100. * train_correct / train_total
                train_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })
                
            except Exception as e:
                print(f"Error in training batch {batch_idx}: {e}")
                continue
        
        if train_total == 0:
            print("No valid training batches processed!")
            break
            
        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = 100. * train_correct / train_total
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [GB-EffNet Val]')
            for batch_idx, (data, target) in enumerate(val_pbar):
                data, target = data.to(device), target.to(device)
                
                try:
                    output = model(data)
                    loss = criterion(output, target)
                    val_loss += loss.item()
                    
                    pred = output.argmax(dim=1, keepdim=True)
                    val_correct += pred.eq(target.view_as(pred)).sum().item()
                    val_total += target.size(0)
                    
                    all_preds.extend(pred.cpu().numpy().flatten())
                    all_targets.extend(target.cpu().numpy())
                    
                    current_acc = 100. * val_correct / val_total
                    val_pbar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Acc': f'{current_acc:.2f}%'
                    })
                    
                except Exception as e:
                    print(f"Error in validation batch {batch_idx}: {e}")
                    continue
        
        if val_total == 0:
            print("No valid validation batches processed!")
            break
            
        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100. * val_correct / val_total
        val_kappa = quadratic_weighted_kappa(all_targets, all_preds)
        
        # Update learning rate
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # Store history
        history['train_losses'].append(avg_train_loss)
        history['val_losses'].append(avg_val_loss)
        history['val_kappas'].append(val_kappa)
        history['val_accuracies'].append(val_accuracy)
        history['learning_rates'].append(current_lr)
        
        # Print epoch results
        print(f'\nEpoch {epoch+1}/{num_epochs} [GB-EffNet]:')
        print(f'  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%')
        print(f'  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%')
        print(f'  Val QWK: {val_kappa:.4f}')
        print(f'  Learning Rate: {current_lr:.8f}')
        
        # Save best model
        if val_kappa > best_kappa:
            best_kappa = val_kappa
            best_model_state = model.state_dict().copy()
            torch.save({
                'model_state_dict': best_model_state,
                'epoch': epoch,
                'best_kappa': best_kappa,
                'history': history
            }, f'best_{model_name}.pth')
            patience_counter = 0
            print(f'  ‚úÖ New best GB EfficientNet model saved! QWK: {best_kappa:.4f}')
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f'\nEarly stopping triggered after {patience} epochs without improvement.')
            break
            
        print('-' * 70)
    
    # Load best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nLoaded best GB EfficientNet model with QWK: {best_kappa:.4f}")
    
    history['best_kappa'] = best_kappa
    history['total_epochs'] = epoch + 1
    
    return model, history

def create_data_loaders(train_csv, img_dir, gb_mean, gb_std, batch_size=16, val_split=0.2):
    """
    Create train and validation data loaders with proper GB transforms
    """
    # Load full dataset
    full_data = pd.read_csv(train_csv)
    
    # Stratified split
    from sklearn.model_selection import train_test_split
    
    train_indices, val_indices = train_test_split(
        range(len(full_data)), 
        test_size=val_split, 
        stratify=full_data.iloc[:, 1], 
        random_state=42
    )
    
    # Create train and val dataframes
    train_data = full_data.iloc[train_indices].reset_index(drop=True)
    val_data = full_data.iloc[val_indices].reset_index(drop=True)
    
    # Save temporary CSV files
    train_data.to_csv('temp_train.csv', index=False)
    val_data.to_csv('temp_val.csv', index=False)
    
    # Create transforms with GB-specific normalization
    train_transform = GBTransforms(
        size=(512, 512), 
        mean=gb_mean, 
        std=gb_std, 
        is_training=True
    )
    val_transform = GBTransforms(
        size=(512, 512), 
        mean=gb_mean, 
        std=gb_std, 
        is_training=False
    )
    
    # Create datasets
    train_dataset = DiabeticRetinopathyGBDataset(
        'temp_train.csv', img_dir, train_transform
    )
    val_dataset = DiabeticRetinopathyGBDataset(
        'temp_val.csv', img_dir, val_transform
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=2, 
        pin_memory=True,
        drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=2, 
        pin_memory=True
    )
    
    print(f"Train loader: {len(train_loader)} batches")
    print(f"Val loader: {len(val_loader)} batches")
    
    return train_loader, val_loader

def plot_training_curves(history, model_name="gb_effnet"):
    """
    Plot training curves for GB EfficientNet model
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    epochs = range(1, len(history['train_losses']) + 1)
    
    # Loss curves
    axes[0, 0].plot(epochs, history['train_losses'], 'b-', label='Training Loss', linewidth=2)
    axes[0, 0].plot(epochs, history['val_losses'], 'r-', label='Validation Loss', linewidth=2)
    axes[0, 0].set_title(f'GB EfficientNet-B2 - Training and Validation Loss')
    axes[0, 0].set_xlabel('Epochs')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # QWK score
    axes[0, 1].plot(epochs, history['val_kappas'], 'g-', label='Validation QWK', linewidth=2)
    axes[0, 1].axhline(y=0.85, color='black', linestyle='--', alpha=0.7, label='Target QWK (0.85)')
    axes[0, 1].axhline(y=0.889, color='red', linestyle='--', alpha=0.7, label='Competition Target (0.889)')
    axes[0, 1].set_title(f'GB EfficientNet-B2 - Validation QWK Score')
    axes[0, 1].set_xlabel('Epochs')
    axes[0, 1].set_ylabel('QWK Score')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Accuracy
    axes[1, 0].plot(epochs, history['val_accuracies'], 'purple', label='Validation Accuracy', linewidth=2)
    axes[1, 0].set_title(f'GB EfficientNet-B2 - Validation Accuracy')
    axes[1, 0].set_xlabel('Epochs')
    axes[1, 0].set_ylabel('Accuracy (%)')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # Learning rate
    axes[1, 1].plot(epochs, history['learning_rates'], 'orange', label='Learning Rate', linewidth=2)
    axes[1, 1].set_title(f'GB EfficientNet-B2 - Learning Rate Schedule')
    axes[1, 1].set_xlabel('Epochs')
    axes[1, 1].set_ylabel('Learning Rate')
    axes[1, 1].set_yscale('log')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{model_name}_training_curves.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"Training curves saved as '{model_name}_training_curves.png'")

def validate_gb_effnet_with_confusion_matrix(model, val_loader):
    """
    Generate detailed validation metrics including confusion matrix for GB EfficientNet
    """
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in tqdm(val_loader, desc="Generating validation metrics"):
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # Calculate metrics
    qwk = quadratic_weighted_kappa(all_targets, all_preds)
    cm = confusion_matrix(all_targets, all_preds)
    
    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'],
                yticklabels=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'])
    plt.title(f'GB EfficientNet-B2 Confusion Matrix (QWK: {qwk:.4f})')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('gb_effnet_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Classification report
    class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative']
    report = classification_report(all_targets, all_preds, target_names=class_names)
    print("\nClassification Report:")
    print(report)
    
    return qwk, cm, report

def main():
    """
    Main training function for GB EfficientNet-B2
    """
    # Configuration optimized for EfficientNet
    config = {
        'train_csv': '/kaggle/input/aptos2019-blindness-detection/train.csv',
        'train_img_dir': '/kaggle/input/aptos2019-blindness-detection/train_images',
        'batch_size': 16,
        'num_epochs': 50,  # More epochs for EfficientNet
        'learning_rate': 2e-4,  # Higher learning rate for EfficientNet
        'num_classes': 5,
        'dropout_rate': 0.4,  # Lower dropout for EfficientNet
        'val_split': 0.2
    }
    
    print("="*70)
    print("DIABETIC RETINOPATHY GB (GREEN-BLUE) CHANNEL EFFICIENTNET-B2 MODEL TRAINING")
    print("="*70)
    
    # Calculate GB normalization statistics
    gb_mean, gb_std = calculate_gb_normalization_stats(
        config['train_csv'], 
        config['train_img_dir'], 
        sample_size=500
    )
    
    print(f"Using GB normalization: mean={gb_mean}, std={gb_std}")
    
    # Create data loaders
    train_loader, val_loader = create_data_loaders(
        config['train_csv'],
        config['train_img_dir'],
        gb_mean,
        gb_std,
        config['batch_size'],
        config['val_split']
    )
    
    # Initialize EfficientNet-B2 model
    model = DREfficientNetB2GBClassifier(
        num_classes=config['num_classes'],
        dropout_rate=config['dropout_rate']
    )
    model.to(device)
    
    # Print model information
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model initialized with {total_params:,} total parameters")
    print(f"Trainable parameters: {trainable_params:,}")
    
    # Train model
    model, history = train_gb_effnet_model(
        model, 
        train_loader, 
        val_loader,
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate'],
        model_name="gb_effnet_model"
    )
    
    # Plot results
    plot_training_curves(history, "gb_effnet_model")
    
    # Detailed validation analysis
    print("\nPerforming detailed validation analysis...")
    final_qwk, cm, report = validate_gb_effnet_with_confusion_matrix(model, val_loader)
    
    # Save final results
    results = {
        'model_type': 'GB Channel EfficientNet-B2 (Green-Blue)',
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'best_qwk': float(history['best_kappa']),
        'final_validation_qwk': float(final_qwk),
        'total_epochs': int(history['total_epochs']),
        'final_val_acc': float(history['val_accuracies'][-1]),
        'model_architecture': 'EfficientNet-B2 + Custom Classifier',
        'input_type': 'GB Channels (2-channel)',
        'preprocessing': 'RGB color space - Green and Blue channels only',
        'image_size': '512x512',
        'gb_normalization': {'mean': gb_mean, 'std': gb_std},
        'config': config,
        'device': str(device),
        'target_achieved': bool(history['best_kappa'] > 0.889)
    }
    
    with open('gb_effnet_model_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n{'='*70}")
    print("GB EFFICIENTNET-B2 MODEL TRAINING COMPLETED")
    print(f"{'='*70}")
    print(f"Best QWK Score: {history['best_kappa']:.4f}")
    print(f"Final Validation QWK: {final_qwk:.4f}")
    print(f"Final Validation Accuracy: {history['val_accuracies'][-1]:.2f}%")
    print(f"Training Epochs: {history['total_epochs']}")
    print(f"Target Achieved: {'‚úÖ Yes' if history['best_kappa'] > 0.889 else '‚ùå No'}")
    print(f"\nFiles saved:")
    print(f"  - Model weights: 'best_gb_effnet_model.pth'")
    print(f"  - Training plots: 'gb_effnet_model_training_curves.png'")
    print(f"  - Confusion matrix: 'gb_effnet_confusion_matrix.png'")
    print(f"  - Results: 'gb_effnet_model_training_results.json'")
    print(f"  - GB Normalization stats: mean={gb_mean}, std={gb_std}")
    
    # Clean up temporary files
    try:
        os.remove('temp_train.csv')
        os.remove('temp_val.csv')
    except:
        pass
    
    return model, history, results, gb_mean, gb_std

if __name__ == "__main__":
    model, history, results, gb_mean, gb_std = main()
    print("\nüéâ GB Channel EfficientNet-B2 Model Training Completed Successfully!")
    
    # Save normalization stats for inference
    norm_stats = {'gb_mean': gb_mean, 'gb_std': gb_std}
    with open('gb_effnet_normalization_stats.json', 'w') as f:
        json.dump(norm_stats, f, indent=2)
    print("GB normalization statistics saved to 'gb_effnet_normalization_stats.json'")
